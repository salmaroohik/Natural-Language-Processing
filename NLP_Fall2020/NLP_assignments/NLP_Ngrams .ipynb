{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Salma Roohi Khayum         Student ID: 801168027***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 1 (2 points): First, you will need to load the words in English and Italian in two separate lists and then create two sets for the words: a training set which contains 80% of the words and a test set which contains 20% of the words (make sure that these sets are created randomly).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_en=pd.read_table('c:\\\\Users\\\\salma\\\\Downloads\\\\CONcreTEXT_trial_EN (1).tsv') #English dataframe \n",
    "df_it=pd.read_table('c:\\\\Users\\\\salma\\\\Downloads\\\\CONcreTEXT_trial_IT (1).tsv') #Italian dataframe\n",
    " \n",
    "import nltk\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "tokenized_list_en=[]\n",
    "word_en=[]\n",
    "for text in df_en['TEXT']:\n",
    "    tokenized_list_en=tokenizer.tokenize(text.lower())\n",
    "    for word in tokenized_list_en:\n",
    "        word_en.append(word)\n",
    "\n",
    "        \n",
    "import random\n",
    "trainingCount = len(word_en)*0.8\n",
    "testCount = len(word_en)*0.2\n",
    "\n",
    "word_train_en = random.sample(word_en,int(trainingCount))\n",
    "word_test_en = random.sample(word_en,int(testCount))\n",
    "\n",
    "\n",
    "################################### Italian #####################################\n",
    "\n",
    "import nltk\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "tokenized_list_en=[]\n",
    "word_it=[]\n",
    "for text in df_it['TEXT']:\n",
    "    tokenized_list_en=tokenizer.tokenize(text.lower())\n",
    "    for word in tokenized_list_en:\n",
    "        word_it.append(word)\n",
    "\n",
    "        \n",
    "import random\n",
    "trainingCount = len(word_it)*0.8\n",
    "testCount = len(word_it)*0.2\n",
    "\n",
    "word_train_it = random.sample(word_it,int(trainingCount))\n",
    "word_test_it = random.sample(word_it,int(testCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bring', 'up', 'academic', 'achievements', 'awards', 'and', 'other', 'milestones', 'in', 'your', 'life', 'please', 'list', 'people', 'you', 'have', 'helped', 'your', 'personal', 'achievements', 'or', 'troublesome', 'times', 'you', 'have', 'overcome', 'add', 'activated', 'carbon', 'straight', 'to', 'your', 'vodka', 'place', 'sensors', 'around', 'your', 'garden', 'and', 'when', 'a', 'cat', 'comes', 'in', 'the', 'motion', 'activates', 'a', 'sensor', 'look', 'for', 'a', 'partner', 'that', 'shares', 'your', 'level', 'of', 'adventure', 'in', 'pursuing', 'new', 'types', 'of', 'experiences', 'most', 'animated', 'action', 'adventure', 'thriller', 'or', 'mystery', 'films', 'and', 'books', 'could', 'work', 'here', 'if', 'people', 'think', 'you', 're', 'phony', 'or', 'putting', 'on', 'airs', 'they', 'll', 'tell', 'from', 'a', 'mile', 'away', 'place', 'your', 'tongue', 'behind', 'the', 'hole', 'in', 'your', 'lips', 'and', 'flick', 'out', 'bits', 'of', 'airs', 'bake', 'the', 'fries', 'until', 'they', 'are', 'crispy', 'and', 'golden', 'brown', 'pour', 'into', 'a', 'souffl√©', 'dish', 'and', 'bake', 'until', 'the', 'top', 'browns', 'and', 'puffs', 'meditation', 'can', 'help', 'your', 'mind', 'and', 'body', 'relax', 'and', 'can', 'help', 'you', 'feel', 'happier', 'they', 'are', 'easier', 'for', 'the', 'human', 'body', 'to', 'digest', 'making', 'your', 'overall', 'experience', 'more', 'pleasant', 'for', 'example', 'you', 'could', 'join', 'a', 'book', 'club', 'or', 'take', 'an', 'art', 'class', 'if', 'you', 'publish', 'a', 'book', 'of', 'poetry', 'or', 'a', 'novel', 'you', 'should', 'give', 'any', 'other', 'publications', 'credit', 'expenses', 'include', 'rent', 'monthly', 'loans', 'car', 'insurance', 'and', 'car', 'loans', 'health', 'insurance', 'and', 'more', 'when', 'you', 'leave', 'the', 'seat', 'of', 'your', 'car', 'you', 'take', 'some', 'of', 'this', 'charge', 'with', 'you', 'painkillers', 'can', 'help', 'both', 'children', 'and', 'adults', 'feel', 'more', 'comfortable', 'as', 'their', 'bodies', 'heal', 'talk', 'to', 'your', 'spouse', 'about', 'child', 'custody', 'and', 'child', 'support', 'al', 'dente', 'pasta', 'is', 'cooked', 'but', 'still', 'firm', 'pasta', 'cooks', 'more', 'quickly', 'and', 'becomes', 'mushy', 'if', 'left', 'in', 'the', 'slow', 'cooker', 'too', 'long', 'contact', 'an', 'emergency', 'response', 'team', 'immediately', 'in', 'the', 'event', 'of', 'a', 'severe', 'trauma', 'or', 'medical', 'crisis', 'decide', 'if', 'you', 'are', 'having', 'an', 'existential', 'crisis', 'regarding', 'love', 'cuddling', 'with', 'pets', 'can', 'make', 'people', 'feel', 'better', 'physical', 'interaction', 'like', 'hand', 'holding', 'or', 'cuddling', 'before', 'you', 'kiss', 'can', 'work', 'as', 'ice', 'breakers', 'complete', 'your', 'grocery', 'shopping', 'decorate', 'the', 'house', 'to', 'ensure', 'you', 've', 'got', 'everything', 'you', 'need', 'cut', 'out', 'the', 'cookies', 'decorate', 'them', 'and', 'continue', 'baking', 'them', 'until', 'you', 'are', 'finished', 'enemies', 'will', 'provoke', 'your', 'anger', 'and', 'destroy', 'your', 'happiness', 'shaded', 'areas', 'tend', 'to', 'be', 'soggier', 'and', 'soggy', 'soil', 'can', 'invite', 'fungi', 'that', 'will', 'destroy', 'the', 'plant', 'a', 'cat', 'or', 'a', 'dog', 'could', 'easily', 'disturb', 'and', 'destroy', 'a', 'nest', 'sleep', 'is', 'often', 'disturbed', 'by', 'depression', 'consider', 'opening', 'the', 'oven', 'door', 'slightly', 'and', 'allowing', 'the', 'pan', 'to', 'cool', 'for', 'a', 'few', 'minutes', 'your', 'thoughts', 'can', 'open', 'up', 'many', 'doors', 'that', 'can', 'lead', 'to', 'friendship', 'trail', 'running', 'combines', 'hills', 'for', 'better', 'cardio', 'training', 'and', 'soft', 'earth', 'for', 'better', 'balance', 'your', 'carbon', 'footprint', 'is', 'your', 'impact', 'on', 'the', 'earth', 's', 'atmosphere', 'eat', 'snacks', 'whenever', 'you', 're', 'hungry', 'and', 'do', 'n', 't', 'skip', 'meals', 'weevils', 'can', 'eat', 'through', 'paper', 'and', 'thin', 'plastic', 'vary', 'the', 'colors', 'if', 'you', 'have', 'a', 'good', 'eye', 'for', 'matching', 'designs', 'you', 'll', 'need', 'felt', 'embroidery', 'thread', 'a', 'needle', 'with', 'a', 'large', 'eye', 'or', 'a', 'sewing', 'machine', 'as', 'your', 'nerves', 'become', 'increasingly', 'more', 'damaged', 'blind', 'spots', 'will', 'develop', 'in', 'your', 'field', 'of', 'vision', 'there', 'are', '30', 'players', 'on', 'the', 'field', 'at', 'any', 'time', 'in', 'a', 'game', 'of', 'rugby', 'do', 'n', 't', 'just', 'flirt', 'over', 'text', 'or', 'send', 'her', 'facebook', 'messages', 'if', 'you', 'actually', 'want', 'to', 'get', 'somewhere', 'try', 'doing', 'something', 'to', 'break', 'the', 'touch', 'jitters', 'away', 'by', 'flirting', 'physically', 'expand', 'your', 'repertoire', 'of', 'brain', 'games', 'if', 'you', 're', 'playing', 'games', 'on', 'a', 'computer', 'with', 'the', 'mouse', 'invest', 'in', 'a', 'wrist', 'cushion', 'there', 'are', 'some', 'clocks', 'that', 'can', 'safely', 'have', 'their', 'minute', 'hand', 'turned', 'counterclockwise', 'upon', 'grabbing', 'the', 'deck', 'turn', 'your', 'entire', 'hand', 'over', 'causing', 'the', 'cards', 'to', 'be', 'face', 'down', 'take', 'your', 'safety', 'pins', 'and', 'attach', 'one', 'card', 'to', 'the', 'head', 'of', 'your', 'bed', 'the', 'pope', 'is', 'also', 'head', 'of', 'the', 'world', 's', 'smallest', 'sovereign', 'state', 'the', 'vatican', 'it', 's', 'no', 'good', 'trying', 'to', 'chase', 'a', 'lizard', 'away', 'when', 'he', 'still', 'has', 'dozens', 'of', 'places', 'to', 'hide', 'many', 'parents', 'try', 'to', 'hide', 'their', 'own', 'negative', 'emotions', 'to', 'protect', 'their', 'children', 'read', 'the', 'company', 's', 'mission', 'statement', 'history', 'and', 'any', 'other', 'information', 'you', 'can', 'find', 'about', 'them', 'when', 'you', 'finished', 'delete', 'the', 'history', 'on', 'your', 'computer', 'that', 'leads', 'to', 'any', 'websites', 'your', 'parents', 'do', 'n', 't', 'want', 'you', 'on', 'invite', 'friends', 'and', 'loved', 'ones', 'to', 'a', 'party', 'or', 'dinner', 'in', 'honor', 'of', 'your', 'child', 'the', 'game', 'of', 'golf', 'is', 'about', 'honesty', 'honor', 'and', 'respect', 'not', 'just', 'hitting', 'a', 'ball', 'around', 'a', 'grassy', 'park', 'hearing', 'a', 'variety', 'of', 'perspectives', 'will', 'help', 'you', 'imagine', 'possibilities', 'so', 'you', 'can', 'make', 'the', 'best', 'decision', 'recall', 'the', 'future', 'you', 've', 'imagined', 'any', 'time', 'you', 'feel', 'stuck', 'or', 'otherwise', 'lost', 'in', 'life', 'find', 'a', 'charity', 'whose', 'work', 'truly', 'inspires', 'you', 'and', 'that', 'you', 'connect', 'with', 'the', 'fertilizer', 'will', 'inspire', 'leafy', 'growth', 'rather', 'than', 'flower', 'growth', 'a', 'loan', 'with', 'annual', 'interest', 'adds', 'the', 'interest', 'rate', 'ten', 'times', 'in', 'ten', 'years', 'if', 'you', 'are', 'a', 'professional', 'maintain', 'active', 'involvement', 'in', 'your', 'field', 'of', 'research', 'interest', 'limit', 'foods', 'and', 'drinks', 'that', 'irritate', 'the', 'bladder', 'sugary', 'foods', 'can', 'irritate', 'the', 'throat', 'before', 'settlement', 'negotiations', 'you', 'should', 'talk', 'with', 'your', 'attorney', 'about', 'how', 'much', 'your', 'lawsuit', 'is', 'worth', 'if', 'you', 'do', 'not', 'have', 'a', 'valid', 'legal', 'claim', 'any', 'lawsuit', 'you', 'bring', 'will', 'be', 'dismissed', 'by', 'the', 'court', 'it', 's', 'important', 'that', 'you', 'learn', 'to', 'trust', 'yourself', 'you', 'll', 'learn', 'how', 'to', 'play', 'by', 'interacting', 'with', 'those', 'more', 'talented', 'than', 'you', 'dress', 'in', 'a', 'professional', 'manner', 'for', 'your', 'interview', 'the', 'manner', 'in', 'which', 'the', 'photo', 'collage', 'will', 'be', 'displayed', 'should', 'be', 'considered', 'when', 'deciding', 'on', 'its', 'size', 'and', 'shape', 'simply', 'watching', 'a', 'movie', 'or', 'listening', 'to', 'music', 'together', 'are', 'relaxing', 'ways', 'to', 'enjoy', 'each', 'others', 'company', 'watch', 'a', 'funny', 'movie', 'or', 'television', 'show', 'do', 'not', 'insult', 'or', 'offend', 'anyone', 'nobody', 'wants', 'to', 'hang', 'out', 'with', 'someone', 'who', 'is', 'so', 'serious', 'that', 'they', 'are', 'constantly', 'afraid', 'of', 'offending', 'them', 'each', 'state', 'has', 'an', 'office', 'of', 'voter', 'affairs', 'when', 'in', 'doubt', 'call', 'doctor', 's', 'office', 'or', 'home', 'health', 'nurse', 'for', 'help', 'many', 'people', 'see', 'their', 'hair', 'as', 'an', 'integral', 'part', 'of', 'their', 'identity', 'mistakes', 'are', 'a', 'part', 'of', 'life', 'if', 'you', 'are', 'making', 'two', 'smaller', 'pizzas', 'cut', 'the', 'dough', 'ball', 'in', 'half', 'if', 'you', 'want', 'to', 'cook', 'large', 'pizzas', 'the', 'floor', 'of', 'the', 'oven', 'must', 'be', 'large', 'as', 'well', 'cars', 'play', 'a', 'big', 'part', 'in', 'polluting', 'the', 'air', 'producing', 'too', 'much', 'trash', 'pollutes', 'the', 'environment', 'focus', 'on', 'quality', 'not', 'quantity', 'when', 'helping', 'others', 'the', 'file', 'type', 'you', 'pick', 'will', 'affect', 'the', 'quality', 'of', 'the', 'image', 'after', 'it', 'is', 'saved', 'use', 'bobby', 'pins', 'to', 'secure', 'the', 'base', 'of', 'the', 'bun', 'to', 'the', 'head', 'you', 'might', 'need', 'to', 'secure', 'a', 'small', 'loan', 'or', 'have', 'savings', 'to', 'start', 'your', 'business', 'oatmeal', 'cleanses', 'moisturizes', 'soothes', 'irritation', 'and', 'relieves', 'itchiness', 'take', 'a', 'few', 'slow', 'deep', 'breaths', 'to', 'help', 'soothe', 'your', 'nerves', 'every', 'relationship', 'can', 'benefit', 'from', 'space', 'and', 'time', 'to', 'miss', 'each', 'other', 'ideally', 'this', 'drive', 'should', 'have', 'enough', 'space', 'to', 'fit', 'all', 'of', 'the', 'data', 'hold', 'the', 'person', 's', 'hand', 'when', 'you', 'cross', 'the', 'street', 'leave', 'the', 'office', 'to', 'walk', 'across', 'the', 'street', 'to', 'get', 'coffee', 'part', 'of', 'loving', 'life', 'is', 'to', 'not', 'be', 'ruled', 'by', 'fear', 'which', 'will', 'suffocate', 'you', 'in', 'unhappiness', 'there', 'is', 'also', 'a', 'potential', 'to', 'suffocate', 'from', 'too', 'much', 'carbon', 'monoxide', 'talk', 'about', 'the', 'places', 'you', 've', 'traveled', 'the', 'customs', 'back', 'home', 'or', 'just', 'stories', 'from', 'your', 'life', 'your', 'voice', 'like', 'any', 'sound', 'travels', 'through', 'various', 'mediums', 'in', 'the', 'form', 'of', 'sound', 'waves', 'if', 'the', 'roses', 'are', 'out', 'of', 'water', 'even', 'for', 'a', 'few', 'minutes', 'they', 'll', 'suffer', 'for', 'it', 'rinse', 'your', 'face', 'with', 'warm', 'water', 'and', 'pat', 'it', 'dry', 'staying', 'mentally', 'strong', 'means', 'winning', 'half', 'the', 'battle', 'to', 'becoming', 'a', 'better', 'person', 'the', 'person', 'who', 'has', 'the', 'highest', 'score', 'wins', 'the', 'game', 'for', 'the', 'most', 'part', 'men', 'and', 'women', 'wear', 'the', 'same', 'types', 'of', 'shoes', 'look', 'at', 'the', 'woman', 'whom', 'you', 'are', 'listening', 'to', 'for', 'as', 'long', 'as', 'you', 'or', 'she', 'is', 'speaking']\n"
     ]
    }
   ],
   "source": [
    "print(word_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finding the occurrence of each letter on the training set of words in both english and italian corpus. unigram_dict_en, unigram_dict_it is respectively used to store the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dict_en={}\n",
    "\n",
    "for word in word_train_en:\n",
    "    for x in (word):\n",
    "        \n",
    "        if x not in unigram_dict_en.keys():\n",
    "            unigram_dict_en[x]=1\n",
    "        else:\n",
    "            unigram_dict_en[x]+=1\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "####################################### Italian ######################################################\n",
    "\n",
    "unigram_dict_it={}\n",
    "\n",
    "for word in word_train_it:\n",
    "    for x in (word):\n",
    "        \n",
    "        if x not in unigram_dict_it.keys():\n",
    "            unigram_dict_it[x]=1\n",
    "        else:\n",
    "            unigram_dict_it[x]+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'h': 219,\n",
       "  'a': 380,\n",
       "  'v': 56,\n",
       "  'e': 560,\n",
       "  'p': 109,\n",
       "  'u': 171,\n",
       "  't': 399,\n",
       "  'i': 325,\n",
       "  'n': 312,\n",
       "  'g': 92,\n",
       "  'q': 4,\n",
       "  'l': 212,\n",
       "  'y': 153,\n",
       "  'o': 421,\n",
       "  's': 276,\n",
       "  'c': 145,\n",
       "  'm': 102,\n",
       "  'w': 70,\n",
       "  'r': 306,\n",
       "  'k': 44,\n",
       "  'f': 124,\n",
       "  'd': 163,\n",
       "  'b': 70,\n",
       "  'z': 7,\n",
       "  'x': 9,\n",
       "  'j': 4,\n",
       "  '3': 1,\n",
       "  '0': 1},\n",
       " {'s': 255,\n",
       "  'i': 607,\n",
       "  '√®': 15,\n",
       "  'd': 190,\n",
       "  'e': 630,\n",
       "  'l': 322,\n",
       "  'a': 558,\n",
       "  'n': 388,\n",
       "  'o': 454,\n",
       "  'p': 187,\n",
       "  'r': 372,\n",
       "  'v': 92,\n",
       "  'u': 194,\n",
       "  'c': 217,\n",
       "  'g': 77,\n",
       "  't': 356,\n",
       "  'z': 46,\n",
       "  'q': 23,\n",
       "  'b': 44,\n",
       "  'h': 33,\n",
       "  'm': 142,\n",
       "  'f': 72,\n",
       "  '√†': 12,\n",
       "  '√π': 7,\n",
       "  '3': 1,\n",
       "  '0': 2,\n",
       "  '√≤': 3,\n",
       "  '√¨': 1,\n",
       "  '√©': 1,\n",
       "  '1': 1,\n",
       "  '2': 2,\n",
       "  '5': 2})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_dict_en, unigram_dict_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4735\n",
      "5306\n"
     ]
    }
   ],
   "source": [
    "sum_train_en=0\n",
    "for x,y in unigram_dict_en.items():\n",
    "    sum_train_en+=y\n",
    "    \n",
    "print(sum_train_en)\n",
    "\n",
    "\n",
    "################################### Italian ################################\n",
    "sum_train_it=0\n",
    "for x,y in unigram_dict_it.items():\n",
    "    sum_train_it+=y\n",
    "    \n",
    "print(sum_train_it)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of each english and italian unigram is calculated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of letters in English train dataset: {'h': 0.04625131995776135, 'a': 0.08025343189017951, 'v': 0.011826821541710665, 'e': 0.11826821541710665, 'p': 0.023020063357972544, 'u': 0.036114044350580785, 't': 0.08426610348468849, 'i': 0.06863780359028511, 'n': 0.0658922914466737, 'g': 0.019429778247096092, 'q': 0.0008447729672650475, 'l': 0.044772967265047516, 'y': 0.032312565997888065, 'o': 0.08891235480464625, 's': 0.05828933474128828, 'c': 0.030623020063357972, 'm': 0.02154171066525871, 'w': 0.014783526927138331, 'r': 0.06462513199577613, 'k': 0.009292502639915523, 'f': 0.026187961985216474, 'd': 0.034424498416050685, 'b': 0.014783526927138331, 'z': 0.0014783526927138332, 'x': 0.0019007391763463568, 'j': 0.0008447729672650475, '3': 0.00021119324181626187, '0': 0.00021119324181626187} \n",
      "\n",
      "\n",
      "probability of letters in Italian train dataset: {'s': 0.04805880135695439, 'i': 0.11439879381831888, '√®': 0.0028269883151149644, 'd': 0.03580851865812288, 'e': 0.11873350923482849, 'l': 0.06068601583113457, 'a': 0.10516396532227666, 'n': 0.0731247644176404, 'o': 0.08556351300414625, 'p': 0.03524312099509989, 'r': 0.07010931021485112, 'v': 0.017338861666038447, 'u': 0.0365623822088202, 'c': 0.040897097625329816, 'g': 0.014511873350923483, 't': 0.06709385601206182, 'z': 0.008669430833019224, 'q': 0.004334715416509612, 'b': 0.008292499057670561, 'h': 0.006219374293252921, 'm': 0.026762156049754994, 'f': 0.013569543912551827, '√†': 0.0022615906520919715, '√π': 0.0013192612137203166, '3': 0.00018846588767433095, '0': 0.0003769317753486619, '√≤': 0.0005653976630229929, '√¨': 0.00018846588767433095, '√©': 0.00018846588767433095, '1': 0.00018846588767433095, '2': 0.0003769317753486619, '5': 0.0003769317753486619} \n"
     ]
    }
   ],
   "source": [
    "probability_Unigram_en={}\n",
    "for x,y in unigram_dict_en.items():\n",
    "    probability_Unigram_en[x]=unigram_dict_en[x]/sum_train_en\n",
    "print(\"probability of letters in English train dataset: {} \".format (probability_Unigram_en) )     \n",
    "print('\\n')\n",
    "############################## Italian ##################################\n",
    "probability_Unigram_it={}\n",
    "for x,y in unigram_dict_it.items():\n",
    "    probability_Unigram_it[x]=unigram_dict_it[x]/sum_train_it\n",
    "print(\"probability of letters in Italian train dataset: {} \".format (probability_Unigram_it) )  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 2 (10 points): Next, we are going to build a unigram model for each language (English and Italian separately). Important note here: the unigrams here refer to character level unigrams. ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigram model for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English unigram characters: ['h', 'a', 'v', 'e', 'p', 'u', 't', 'i', 'n', 'g', 'q', 'l', 'y', 'o', 's', 'c', 'm', 'w', 'r', 'k', 'f', 'd', 'b', 'z', 'x', 'j', '3', '0']\n",
      "Italian unigram characters: ['s', 'i', '√®', 'd', 'e', 'l', 'a', 'n', 'o', 'p', 'r', 'v', 'u', 'c', 'g', 't', 'z', 'q', 'b', 'h', 'm', 'f', '√†', '√π', '3', '0', '√≤', '√¨', '√©', '1', '2', '5']\n"
     ]
    }
   ],
   "source": [
    "unigram_en=probability_Unigram_en.keys()\n",
    "print(\"English unigram characters: {}\".format (list(unigram_en)))\n",
    "\n",
    "unigram_it=probability_Unigram_it.keys()\n",
    "print(\"Italian unigram characters: {}\".format (list(unigram_it)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "        \n",
    "def probability_word_en(word):\n",
    "    probability_word_vr=1\n",
    "    for x in word:\n",
    "        if x in probability_Unigram_en.keys():\n",
    "            probability_word_vr=probability_word_vr*probability_Unigram_en[x]\n",
    "            #print(probability_word_vr,probability_Unigram_en[x],x)\n",
    "        else:\n",
    "            probability_word_vr*=1\n",
    "            #print(probability_word_vr,probability_Unigram_en[x],x)\n",
    "    return(probability_word_vr)\n",
    "\n",
    "def probability_word_it(word):\n",
    "    probability_word_vr_it=1\n",
    "    for x in word:\n",
    "        if x in probability_Unigram_it.keys():\n",
    "            probability_word_vr_it=probability_word_vr_it*probability_Unigram_it[x]\n",
    "            #print(probability_word_vr,probability_Unigram_en[x],x)\n",
    "        else:\n",
    "            probability_word_vr_it*=1\n",
    "            #print(probability_word_vr,probability_Unigram_en[x],x)\n",
    "    return(probability_word_vr_it)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in the English test set, calculated the probability assigned to that string by the English vs. Italian unigram model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_dict_test_en={}\n",
    "for word in word_test_en:\n",
    "    final_dict_test_en[word]= probability_word_en(word)\n",
    "    \n",
    "final_dict_test_it={}\n",
    "for word in word_test_en:\n",
    "    final_dict_test_it[word]= probability_word_it(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in the Italian test set, calculated the probability assigned to that string by the English vs. Italian unigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict_test_en_it_words={}\n",
    "for word in word_test_it:\n",
    "    final_dict_test_en_it_words[word]= probability_word_en(word)\n",
    "    \n",
    "final_dict_test_it_it_words={}\n",
    "for word in word_test_it:\n",
    "    final_dict_test_it_it_words[word]= probability_word_it(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language is labeled based on the probabilities obtained for english test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lang_en={}\n",
    "for word in word_test_en:\n",
    "    if final_dict_test_en[word]>final_dict_test_it[word]:\n",
    "        dict_lang_en[word]='English'\n",
    "    else:\n",
    "        dict_lang_en[word]='Italian'\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language is labeled based on the probabilities obtained for Italian test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lang_it={}\n",
    "for word in word_test_it:\n",
    "    if final_dict_test_en_it_words[word]>final_dict_test_it_it_words[word]:\n",
    "        dict_lang_it[word]='English'\n",
    "    else:\n",
    "        dict_lang_it[word]='Italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the Unigram model is calculated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the unigram model for english is: 29.05027932960894\n"
     ]
    }
   ],
   "source": [
    "count_en=0\n",
    "count_it=0\n",
    "for x,y in dict_lang_en.items():\n",
    "    if y=='English':\n",
    "        count_en+=1\n",
    "        \n",
    "accuracy_unigram_model_en= (count_en/len(dict_lang_en))*100       \n",
    "print(\"Accuracy of the unigram model for english is: {}\".format(accuracy_unigram_model_en)  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the unigram model for Italian is: 79.78142076502732\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_it=0\n",
    "for x,y in dict_lang_it.items():\n",
    "    if y=='Italian':\n",
    "        count_it+=1\n",
    "        \n",
    "accuracy_unigram_model_it= (count_it/len(dict_lang_it))*100       \n",
    "print(\"Accuracy of the unigram model for Italian is: {}\".format(accuracy_unigram_model_it)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 3 (4 points): Repeat the entire Question 2, but instead for a bigram character model. How did the accuracies change? Did they increase or decrease? Is a bigram character-level language model better at distinguishing language than a unigram character-level language model? Type an *original* answer with at least 50 words. ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of bigram model by using NLTK libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "def ngramer(df, gram ):\n",
    "    temp = []\n",
    "    final =[]\n",
    "    for word in df:\n",
    "        temp.append(list(ngrams(word, gram , pad_left=True, pad_right=True, left_pad_symbol=\"_\", right_pad_symbol=\"_\")))\n",
    "    temp = [word for sublist in temp for word in sublist]\n",
    "    n_grams = temp\n",
    "    for i, v in enumerate(temp):\n",
    "        n_grams[i] = ''.join(v)\n",
    "                \n",
    "    return temp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_en=ngramer(word_train_en,2)\n",
    "bi_it=ngramer(word_train_it,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_count_bi_en={}#occurrence of each english bigram is tabulted here\n",
    "for x in bi_en:        \n",
    "    if x not in dict_count_bi_en.keys():\n",
    "        dict_count_bi_en[x]=1\n",
    "    else:\n",
    "        dict_count_bi_en[x]+=1    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_count_bi_it={}#occurrence of each Italian bigram is tabulted here\n",
    "for x in bi_it:        \n",
    "    if x not in dict_count_bi_it.keys():\n",
    "        dict_count_bi_it[x]=1\n",
    "    else:\n",
    "        dict_count_bi_it[x]+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#***I have used Laplace's smoothing to improve the accuracy of the model by targetting the gram keys which are \n",
    "##                                                                                     not found in the model.\n",
    "\n",
    "# https://web.stanford.edu/~jurafsky/slp3/3.pdf explains more about Laplace smoothing which i referred for my implementation.\n",
    "\n",
    "def probability_word_bi_en(word):#probability of the given word is calculated with repect to the English bigram model\n",
    "    prob=1\n",
    "    #pro_bi_word_en={}\n",
    "    bi=[word[i:i+2] for i in range (len(word))]\n",
    "    #print(bi)\n",
    "    for v in bi:\n",
    "    \n",
    "        if v in dict_count_bi_en.keys():\n",
    "            prob*=dict_count_bi_en[v]+1/unigram_dict_en[v[0]]+len(word_train_en)\n",
    "            #print(v,prob,dict_count_bi_en[v],unigram_dict_en[v[0]],v[0])\n",
    "        \n",
    "    #pro_bi_word_en[word]=prob\n",
    "    return prob  \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#***I have used Laplace's smoothing to improve the accuracy of the model by targetting the gram keys which are \n",
    "##                                                                                     not found in the model.\n",
    "\n",
    "# https://web.stanford.edu/~jurafsky/slp3/3.pdf explains more about Laplace smoothing which i referred for my implementation.\n",
    "\n",
    "def probability_word_bi_it(word):#probability of the given word is calculated with repect to the Italian bigram model\n",
    "    prob=1\n",
    "    #pro_bi_word_en={}\n",
    "    bi=[word[i:i+2] for i in range (len(word))]\n",
    "    for v in bi:\n",
    "    \n",
    "        if v in dict_count_bi_it.keys():\n",
    "            prob*=dict_count_bi_it[v]+1/unigram_dict_it[v[0]]+len(word_train_it)\n",
    "            #print(x,prob,dict_count_bi_en[v],unigram_dict_en[v[0]],v[0])\n",
    "        \n",
    "    #pro_bi_word_en[word]=prob\n",
    "    return prob  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  For each word in the English test set, calculated the probability assigned to that string by the English vs. Italian bigram  model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_bi_word_en={}\n",
    "for  word in word_test_en:\n",
    "    pro_bi_word_en[word]=probability_word_bi_en(word)\n",
    "    \n",
    "\n",
    "\n",
    "pro_bi_word_it={}\n",
    "for  word in word_test_en:\n",
    "    pro_bi_word_it[word]=probability_word_bi_it(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language is labeled based on the probabilities obtained for english test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lang_bi_en={}\n",
    "for word in word_test_en:\n",
    "    if pro_bi_word_en[word]>pro_bi_word_it[word]:\n",
    "        dict_lang_bi_en[word]='English'\n",
    "    else:\n",
    "        dict_lang_bi_en[word]='Italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of bigram model is computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram model is: 73.18435754189943\n",
      "Accuracy of bigram model is: 73.18435754189943 Accuracy of unigram model is:29.05027932960894\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_bi_en=0\n",
    "count_bi_it=0\n",
    "for x,y in dict_lang_bi_en.items():\n",
    "    if y=='English':\n",
    "        count_bi_en+=1\n",
    "        \n",
    "accuracy_bigram_model=(count_bi_en/len(dict_lang_bi_en))*100        \n",
    "print(\"Accuracy of bigram model is: {}\".format(accuracy_bigram_model))  \n",
    "print(\"Accuracy of bigram model is: {0} Accuracy of unigram model is:{1}\".format(accuracy_bigram_model,accuracy_unigram_model_en))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy of Bigram model outperforms the accuracy of the unigram model significntly. This is becuse in unigram model, individual letter occurence and its probability is used to find the word probability and thus find the language to which the word belongs. In this case the unique letters thus found as unigrams in english corpus are also found in the italian unigrams which is ambiguous. This leads to the miss labeling of words as Italian and the accuracy is less in unigram model. In Bigram model the bigrams are quite distinct and it is not possible to have all the english bigrams in the italian set of bigrams. Thus there is less chance of miss interpretation and the accuracy improves**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 4 (4 points): Repeat the entire Question 3, but this time for a trigram character model. Is a trigram character-level language model better at distinguishing language than a bigram character-level language model? Type an original answer with at least 50 words. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_en=ngramer(word_train_en,3)\n",
    "tri_it=ngramer(word_train_it,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__h',\n",
       " '_ha',\n",
       " 'hav',\n",
       " 'ave',\n",
       " 've_',\n",
       " 'e__',\n",
       " '__p',\n",
       " '_pu',\n",
       " 'put',\n",
       " 'utt',\n",
       " 'tti',\n",
       " 'tin',\n",
       " 'ing',\n",
       " 'ng_',\n",
       " 'g__',\n",
       " '__q',\n",
       " '_qu',\n",
       " 'qua',\n",
       " 'ual',\n",
       " 'ali',\n",
       " 'lit',\n",
       " 'ity',\n",
       " 'ty_',\n",
       " 'y__',\n",
       " '__l',\n",
       " '_lo',\n",
       " 'los',\n",
       " 'ost',\n",
       " 'st_',\n",
       " 't__',\n",
       " '__c',\n",
       " '_co',\n",
       " 'com',\n",
       " 'omp',\n",
       " 'mpa',\n",
       " 'pan',\n",
       " 'any',\n",
       " 'ny_',\n",
       " 'y__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'ou_',\n",
       " 'u__',\n",
       " '__a',\n",
       " '_a_',\n",
       " 'a__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'ou_',\n",
       " 'u__',\n",
       " '__w',\n",
       " '_wo',\n",
       " 'wor',\n",
       " 'ork',\n",
       " 'rk_',\n",
       " 'k__',\n",
       " '__s',\n",
       " '_st',\n",
       " 'sta',\n",
       " 'tat',\n",
       " 'ate',\n",
       " 'tem',\n",
       " 'eme',\n",
       " 'men',\n",
       " 'ent',\n",
       " 'nt_',\n",
       " 't__',\n",
       " '__f',\n",
       " '_fe',\n",
       " 'few',\n",
       " 'ew_',\n",
       " 'w__',\n",
       " '__p',\n",
       " '_pe',\n",
       " 'per',\n",
       " 'ers',\n",
       " 'rso',\n",
       " 'son',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__l',\n",
       " '_li',\n",
       " 'lif',\n",
       " 'ife',\n",
       " 'fe_',\n",
       " 'e__',\n",
       " '__o',\n",
       " '_or',\n",
       " 'or_',\n",
       " 'r__',\n",
       " '__d',\n",
       " '_do',\n",
       " 'doc',\n",
       " 'oct',\n",
       " 'cto',\n",
       " 'tor',\n",
       " 'or_',\n",
       " 'r__',\n",
       " '__a',\n",
       " '_aw',\n",
       " 'awa',\n",
       " 'way',\n",
       " 'ay_',\n",
       " 'y__',\n",
       " '__a',\n",
       " '_ab',\n",
       " 'abo',\n",
       " 'bou',\n",
       " 'out',\n",
       " 'ut_',\n",
       " 't__',\n",
       " '__i',\n",
       " '_in',\n",
       " 'ins',\n",
       " 'nsu',\n",
       " 'sul',\n",
       " 'ult',\n",
       " 'lt_',\n",
       " 't__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__d',\n",
       " '_do',\n",
       " 'dou',\n",
       " 'oub',\n",
       " 'ubt',\n",
       " 'bt_',\n",
       " 't__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'thi',\n",
       " 'his',\n",
       " 'is_',\n",
       " 's__',\n",
       " '__o',\n",
       " '_or',\n",
       " 'or_',\n",
       " 'r__',\n",
       " '__p',\n",
       " '_pe',\n",
       " 'peo',\n",
       " 'eop',\n",
       " 'opl',\n",
       " 'ple',\n",
       " 'le_',\n",
       " 'e__',\n",
       " '__a',\n",
       " '_ad',\n",
       " 'adv',\n",
       " 'dve',\n",
       " 'ven',\n",
       " 'ent',\n",
       " 'ntu',\n",
       " 'tur',\n",
       " 'ure',\n",
       " 're_',\n",
       " 'e__',\n",
       " '__t',\n",
       " '_tr',\n",
       " 'tro',\n",
       " 'rou',\n",
       " 'oub',\n",
       " 'ubl',\n",
       " 'ble',\n",
       " 'les',\n",
       " 'eso',\n",
       " 'som',\n",
       " 'ome',\n",
       " 'me_',\n",
       " 'e__',\n",
       " '__w',\n",
       " '_wi',\n",
       " 'wit',\n",
       " 'ith',\n",
       " 'th_',\n",
       " 'h__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__n',\n",
       " '_ne',\n",
       " 'nes',\n",
       " 'est',\n",
       " 'st_',\n",
       " 't__',\n",
       " '__n',\n",
       " '_n_',\n",
       " 'n__',\n",
       " '__i',\n",
       " '_id',\n",
       " 'ide',\n",
       " 'dea',\n",
       " 'eal',\n",
       " 'all',\n",
       " 'lly',\n",
       " 'ly_',\n",
       " 'y__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'be_',\n",
       " 'e__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__c',\n",
       " '_ca',\n",
       " 'cat',\n",
       " 'at_',\n",
       " 't__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__g',\n",
       " '_ga',\n",
       " 'gar',\n",
       " 'ard',\n",
       " 'rde',\n",
       " 'den',\n",
       " 'en_',\n",
       " 'n__',\n",
       " '__m',\n",
       " '_ma',\n",
       " 'mat',\n",
       " 'atc',\n",
       " 'tch',\n",
       " 'chi',\n",
       " 'hin',\n",
       " 'ing',\n",
       " 'ng_',\n",
       " 'g__',\n",
       " '__a',\n",
       " '_ad',\n",
       " 'adv',\n",
       " 'dve',\n",
       " 'ven',\n",
       " 'ent',\n",
       " 'ntu',\n",
       " 'tur',\n",
       " 'ure',\n",
       " 're_',\n",
       " 'e__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'ou_',\n",
       " 'u__',\n",
       " '__n',\n",
       " '_n_',\n",
       " 'n__',\n",
       " '__t',\n",
       " '_ty',\n",
       " 'typ',\n",
       " 'ype',\n",
       " 'pes',\n",
       " 'es_',\n",
       " 's__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'our',\n",
       " 'ur_',\n",
       " 'r__',\n",
       " '__f',\n",
       " '_fo',\n",
       " 'for',\n",
       " 'or_',\n",
       " 'r__',\n",
       " '__a',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__s',\n",
       " '_so',\n",
       " 'so_',\n",
       " 'o__',\n",
       " '__o',\n",
       " '_or',\n",
       " 'or_',\n",
       " 'r__',\n",
       " '__f',\n",
       " '_fo',\n",
       " 'foo',\n",
       " 'ood',\n",
       " 'ods',\n",
       " 'ds_',\n",
       " 's__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'be_',\n",
       " 'e__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'our',\n",
       " 'ur_',\n",
       " 'r__',\n",
       " '__s',\n",
       " '_so',\n",
       " 'sog',\n",
       " 'ogg',\n",
       " 'ggy',\n",
       " 'gy_',\n",
       " 'y__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'be_',\n",
       " 'e__',\n",
       " '__m',\n",
       " '_ma',\n",
       " 'man',\n",
       " 'ann',\n",
       " 'nne',\n",
       " 'ner',\n",
       " 'er_',\n",
       " 'r__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'ou_',\n",
       " 'u__',\n",
       " '__o',\n",
       " '_ot',\n",
       " 'oth',\n",
       " 'the',\n",
       " 'her',\n",
       " 'ers',\n",
       " 'rs_',\n",
       " 's__',\n",
       " '__a',\n",
       " '_a_',\n",
       " 'a__',\n",
       " '__s',\n",
       " '_su',\n",
       " 'suf',\n",
       " 'uff',\n",
       " 'ffo',\n",
       " 'foc',\n",
       " 'oca',\n",
       " 'cat',\n",
       " 'ate',\n",
       " 'te_',\n",
       " 'e__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'ou_',\n",
       " 'u__',\n",
       " '__u',\n",
       " '_us',\n",
       " 'use',\n",
       " 'se_',\n",
       " 'e__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'off',\n",
       " 'ffi',\n",
       " 'fic',\n",
       " 'ice',\n",
       " 'ce_',\n",
       " 'e__',\n",
       " '__t',\n",
       " '_te',\n",
       " 'tea',\n",
       " 'eam',\n",
       " 'am_',\n",
       " 'm__',\n",
       " '__v',\n",
       " '_ve',\n",
       " 've_',\n",
       " 'e__',\n",
       " '__s',\n",
       " '_se',\n",
       " 'sec',\n",
       " 'ecu',\n",
       " 'cur',\n",
       " 'ure',\n",
       " 're_',\n",
       " 'e__',\n",
       " '__c',\n",
       " '_ca',\n",
       " 'car',\n",
       " 'ard',\n",
       " 'rd_',\n",
       " 'd__',\n",
       " '__p',\n",
       " '_pa',\n",
       " 'pas',\n",
       " 'ast',\n",
       " 'sta',\n",
       " 'ta_',\n",
       " 'a__',\n",
       " '__s',\n",
       " '_so',\n",
       " 'so_',\n",
       " 'o__',\n",
       " '__o',\n",
       " '_or',\n",
       " 'or_',\n",
       " 'r__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'our',\n",
       " 'ur_',\n",
       " 'r__',\n",
       " '__f',\n",
       " '_fe',\n",
       " 'fer',\n",
       " 'ert',\n",
       " 'rti',\n",
       " 'til',\n",
       " 'ili',\n",
       " 'liz',\n",
       " 'ize',\n",
       " 'zer',\n",
       " 'er_',\n",
       " 'r__',\n",
       " '__d',\n",
       " '_do',\n",
       " 'dou',\n",
       " 'oug',\n",
       " 'ugh',\n",
       " 'gh_',\n",
       " 'h__',\n",
       " '__o',\n",
       " '_or',\n",
       " 'or_',\n",
       " 'r__',\n",
       " '__c',\n",
       " '_ca',\n",
       " 'cat',\n",
       " 'at_',\n",
       " 't__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'ou_',\n",
       " 'u__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'bet',\n",
       " 'ett',\n",
       " 'tte',\n",
       " 'ter',\n",
       " 'er_',\n",
       " 'r__',\n",
       " '__e',\n",
       " '_ex',\n",
       " 'exp',\n",
       " 'xpe',\n",
       " 'pen',\n",
       " 'ens',\n",
       " 'nse',\n",
       " 'ses',\n",
       " 'es_',\n",
       " 's__',\n",
       " '__m',\n",
       " '_ma',\n",
       " 'mak',\n",
       " 'aki',\n",
       " 'kin',\n",
       " 'ing',\n",
       " 'ng_',\n",
       " 'g__',\n",
       " '__w',\n",
       " '_wh',\n",
       " 'whe',\n",
       " 'hen',\n",
       " 'en_',\n",
       " 'n__',\n",
       " '__w',\n",
       " '_wa',\n",
       " 'wal',\n",
       " 'alk',\n",
       " 'lk_',\n",
       " 'k__',\n",
       " '__c',\n",
       " '_co',\n",
       " 'com',\n",
       " 'omp',\n",
       " 'mpu',\n",
       " 'put',\n",
       " 'ute',\n",
       " 'ter',\n",
       " 'er_',\n",
       " 'r__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'ou_',\n",
       " 'u__',\n",
       " '__s',\n",
       " '_so',\n",
       " 'sou',\n",
       " 'oun',\n",
       " 'und',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__a',\n",
       " '_a_',\n",
       " 'a__',\n",
       " '__s',\n",
       " '_s_',\n",
       " 's__',\n",
       " '__d',\n",
       " '_da',\n",
       " 'dam',\n",
       " 'ama',\n",
       " 'mag',\n",
       " 'age',\n",
       " 'ged',\n",
       " 'ed_',\n",
       " 'd__',\n",
       " '__w',\n",
       " '_wi',\n",
       " 'wit',\n",
       " 'ith',\n",
       " 'th_',\n",
       " 'h__',\n",
       " '__p',\n",
       " '_pr',\n",
       " 'pro',\n",
       " 'rod',\n",
       " 'odu',\n",
       " 'duc',\n",
       " 'uci',\n",
       " 'cin',\n",
       " 'ing',\n",
       " 'ng_',\n",
       " 'g__',\n",
       " '__t',\n",
       " '_to',\n",
       " 'to_',\n",
       " 'o__',\n",
       " '__p',\n",
       " '_pl',\n",
       " 'pla',\n",
       " 'lay',\n",
       " 'ay_',\n",
       " 'y__',\n",
       " '__b',\n",
       " '_be',\n",
       " 'bet',\n",
       " 'ett',\n",
       " 'tte',\n",
       " 'ter',\n",
       " 'er_',\n",
       " 'r__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'our',\n",
       " 'ur_',\n",
       " 'r__',\n",
       " '__h',\n",
       " '_hi',\n",
       " 'hid',\n",
       " 'ide',\n",
       " 'de_',\n",
       " 'e__',\n",
       " '__c',\n",
       " '_ch',\n",
       " 'chi',\n",
       " 'hil',\n",
       " 'ild',\n",
       " 'ldr',\n",
       " 'dre',\n",
       " 'ren',\n",
       " 'en_',\n",
       " 'n__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__t',\n",
       " '_tr',\n",
       " 'tra',\n",
       " 'rau',\n",
       " 'aum',\n",
       " 'uma',\n",
       " 'ma_',\n",
       " 'a__',\n",
       " '__a',\n",
       " '_a_',\n",
       " 'a__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__i',\n",
       " '_it',\n",
       " 'its',\n",
       " 'ts_',\n",
       " 's__',\n",
       " '__a',\n",
       " '_af',\n",
       " 'aff',\n",
       " 'ffe',\n",
       " 'fec',\n",
       " 'ect',\n",
       " 'ct_',\n",
       " 't__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'ou_',\n",
       " 'u__',\n",
       " '__v',\n",
       " '_ve',\n",
       " 've_',\n",
       " 'e__',\n",
       " '__s',\n",
       " '_sl',\n",
       " 'slo',\n",
       " 'low',\n",
       " 'ow_',\n",
       " 'w__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'off',\n",
       " 'ffi',\n",
       " 'fic',\n",
       " 'ice',\n",
       " 'ce_',\n",
       " 'e__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'our',\n",
       " 'ur_',\n",
       " 'r__',\n",
       " '__s',\n",
       " '_st',\n",
       " 'stu',\n",
       " 'tuc',\n",
       " 'uck',\n",
       " 'ck_',\n",
       " 'k__',\n",
       " '__t',\n",
       " '_to',\n",
       " 'to_',\n",
       " 'o__',\n",
       " '__p',\n",
       " '_pu',\n",
       " 'pub',\n",
       " 'ubl',\n",
       " 'bli',\n",
       " 'lis',\n",
       " 'ish',\n",
       " 'sh_',\n",
       " 'h__',\n",
       " '__w',\n",
       " '_wh',\n",
       " 'whe',\n",
       " 'hen',\n",
       " 'en_',\n",
       " 'n__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'ou_',\n",
       " 'u__',\n",
       " '__h',\n",
       " '_ho',\n",
       " 'hon',\n",
       " 'ono',\n",
       " 'nor',\n",
       " 'or_',\n",
       " 'r__',\n",
       " '__w',\n",
       " '_wi',\n",
       " 'wit',\n",
       " 'ith',\n",
       " 'th_',\n",
       " 'h__',\n",
       " '__m',\n",
       " '_mu',\n",
       " 'mus',\n",
       " 'ust',\n",
       " 'st_',\n",
       " 't__',\n",
       " '__c',\n",
       " '_ca',\n",
       " 'car',\n",
       " 'arb',\n",
       " 'rbo',\n",
       " 'bon',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__s',\n",
       " '_se',\n",
       " 'sen',\n",
       " 'ens',\n",
       " 'nso',\n",
       " 'sor',\n",
       " 'or_',\n",
       " 'r__',\n",
       " '__b',\n",
       " '_br',\n",
       " 'bri',\n",
       " 'rin',\n",
       " 'ing',\n",
       " 'ng_',\n",
       " 'g__',\n",
       " '__a',\n",
       " '_ar',\n",
       " 'are',\n",
       " 're_',\n",
       " 'e__',\n",
       " '__o',\n",
       " '_ot',\n",
       " 'oth',\n",
       " 'the',\n",
       " 'her',\n",
       " 'ers',\n",
       " 'rs_',\n",
       " 's__',\n",
       " '__p',\n",
       " '_pa',\n",
       " 'par',\n",
       " 'art',\n",
       " 'rt_',\n",
       " 't__',\n",
       " '__d',\n",
       " '_do',\n",
       " 'dow',\n",
       " 'own',\n",
       " 'wn_',\n",
       " 'n__',\n",
       " '__w',\n",
       " '_wa',\n",
       " 'wat',\n",
       " 'atc',\n",
       " 'tch',\n",
       " 'ch_',\n",
       " 'h__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'off',\n",
       " 'ffe',\n",
       " 'fen',\n",
       " 'end',\n",
       " 'ndi',\n",
       " 'din',\n",
       " 'ing',\n",
       " 'ng_',\n",
       " 'g__',\n",
       " '__h',\n",
       " '_ho',\n",
       " 'hou',\n",
       " 'ous',\n",
       " 'use',\n",
       " 'se_',\n",
       " 'e__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__a',\n",
       " '_ac',\n",
       " 'ach',\n",
       " 'chi',\n",
       " 'hie',\n",
       " 'iev',\n",
       " 'eve',\n",
       " 'vem',\n",
       " 'eme',\n",
       " 'men',\n",
       " 'ent',\n",
       " 'nts',\n",
       " 'ts_',\n",
       " 's__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__m',\n",
       " '_me',\n",
       " 'men',\n",
       " 'en_',\n",
       " 'n__',\n",
       " '__d',\n",
       " '_do',\n",
       " 'doo',\n",
       " 'oor',\n",
       " 'ors',\n",
       " 'rs_',\n",
       " 's__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " 'f__',\n",
       " '__a',\n",
       " '_ab',\n",
       " 'abo',\n",
       " 'bou',\n",
       " 'out',\n",
       " 'ut_',\n",
       " 't__',\n",
       " '__o',\n",
       " '_on',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__c',\n",
       " '_ca',\n",
       " 'can',\n",
       " 'an_',\n",
       " 'n__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'ou_',\n",
       " 'u__',\n",
       " '__g',\n",
       " '_go',\n",
       " 'gol',\n",
       " 'olf',\n",
       " 'lf_',\n",
       " 'f__',\n",
       " '__s',\n",
       " '_si',\n",
       " 'siz',\n",
       " 'ize',\n",
       " 'ze_',\n",
       " 'e__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__s',\n",
       " '_s_',\n",
       " 's__',\n",
       " '__p',\n",
       " '_pr',\n",
       " 'pro',\n",
       " 'rof',\n",
       " 'ofe',\n",
       " 'fes',\n",
       " 'ess',\n",
       " 'ssi',\n",
       " 'sio',\n",
       " 'ion',\n",
       " 'ona',\n",
       " 'nal',\n",
       " 'al_',\n",
       " 'l__',\n",
       " '__f',\n",
       " '_fi',\n",
       " 'fin',\n",
       " 'ind',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__f',\n",
       " '_fe',\n",
       " 'fea',\n",
       " 'ear',\n",
       " 'ar_',\n",
       " 'r__',\n",
       " '__s',\n",
       " '_so',\n",
       " 'sou',\n",
       " 'oun',\n",
       " 'und',\n",
       " 'nd_',\n",
       " 'd__',\n",
       " '__p',\n",
       " '_pl',\n",
       " 'ple',\n",
       " 'lea',\n",
       " 'eas',\n",
       " 'asa',\n",
       " 'san',\n",
       " 'ant',\n",
       " 'nt_',\n",
       " 't__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'her',\n",
       " 'ere',\n",
       " 're_',\n",
       " 'e__',\n",
       " '__t',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " 'e__',\n",
       " '__o',\n",
       " '_ow',\n",
       " 'own',\n",
       " 'wn_',\n",
       " 'n__',\n",
       " '__h',\n",
       " '_ho',\n",
       " 'hom',\n",
       " 'ome',\n",
       " 'me_',\n",
       " 'e__',\n",
       " '__c',\n",
       " '_ca',\n",
       " 'can',\n",
       " 'an_',\n",
       " 'n__',\n",
       " '__c',\n",
       " '_cr',\n",
       " 'cre',\n",
       " 'red',\n",
       " 'edi',\n",
       " 'dit',\n",
       " 'it_',\n",
       " 't__',\n",
       " '__s',\n",
       " '_so',\n",
       " 'som',\n",
       " 'ome',\n",
       " 'me_',\n",
       " 'e__',\n",
       " '__m',\n",
       " '_me',\n",
       " 'med',\n",
       " 'edi',\n",
       " 'dit',\n",
       " 'ita',\n",
       " 'tat',\n",
       " 'ati',\n",
       " 'tio',\n",
       " 'ion',\n",
       " 'on_',\n",
       " 'n__',\n",
       " '__a',\n",
       " '_al',\n",
       " 'all',\n",
       " 'llo',\n",
       " 'low',\n",
       " 'owi',\n",
       " 'win',\n",
       " 'ing',\n",
       " 'ng_',\n",
       " 'g__',\n",
       " '__r',\n",
       " '_re',\n",
       " 'rec',\n",
       " 'eca',\n",
       " 'cal',\n",
       " 'all',\n",
       " 'll_',\n",
       " 'l__',\n",
       " '__y',\n",
       " '_yo',\n",
       " 'you',\n",
       " 'our',\n",
       " 'ur_',\n",
       " 'r__',\n",
       " '__s',\n",
       " '_sa',\n",
       " 'saf',\n",
       " 'afe',\n",
       " 'fet',\n",
       " 'ety',\n",
       " 'ty_',\n",
       " 'y__',\n",
       " '__v',\n",
       " '_va',\n",
       " 'var',\n",
       " 'ari',\n",
       " 'rie',\n",
       " 'iet',\n",
       " 'ety',\n",
       " 'ty_',\n",
       " 'y__',\n",
       " '__i',\n",
       " '_in',\n",
       " 'in_',\n",
       " 'n__',\n",
       " '__o',\n",
       " '_of',\n",
       " 'of_',\n",
       " ...]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_count_tri_en={}#occurrence of each English trigram is tabulted here\n",
    "for x in tri_en:        \n",
    "    if x not in dict_count_tri_en.keys():\n",
    "        dict_count_tri_en[x]=1\n",
    "    else:\n",
    "        dict_count_tri_en[x]+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__s': 89,\n",
       " '_si': 9,\n",
       " 'si_': 13,\n",
       " 'i__': 216,\n",
       " '__√®': 15,\n",
       " '_√®_': 15,\n",
       " '√®__': 15,\n",
       " '__d': 118,\n",
       " '_de': 41,\n",
       " 'del': 28,\n",
       " 'ell': 29,\n",
       " 'lla': 22,\n",
       " 'la_': 48,\n",
       " 'a__': 191,\n",
       " '__i': 84,\n",
       " '_in': 45,\n",
       " 'in_': 20,\n",
       " 'n__': 69,\n",
       " '__o': 24,\n",
       " '_o_': 9,\n",
       " 'o__': 180,\n",
       " '__p': 118,\n",
       " '_pr': 35,\n",
       " 'pro': 13,\n",
       " 'rov': 1,\n",
       " 'ove': 2,\n",
       " 've_': 2,\n",
       " 'e__': 266,\n",
       " '__u': 46,\n",
       " '_un': 37,\n",
       " 'una': 14,\n",
       " 'na_': 22,\n",
       " 'un_': 25,\n",
       " '__c': 91,\n",
       " '_co': 43,\n",
       " 'con': 25,\n",
       " 'oni': 7,\n",
       " 'nig': 1,\n",
       " 'igl': 5,\n",
       " 'gli': 11,\n",
       " 'li_': 12,\n",
       " '_di': 45,\n",
       " 'di_': 48,\n",
       " '__e': 50,\n",
       " '_e_': 26,\n",
       " 'pre': 20,\n",
       " 'res': 7,\n",
       " 'ese': 2,\n",
       " 'sen': 5,\n",
       " 'ent': 41,\n",
       " 'nta': 5,\n",
       " 'taz': 1,\n",
       " 'azi': 8,\n",
       " 'zio': 17,\n",
       " 'ion': 19,\n",
       " 'one': 13,\n",
       " 'ne_': 28,\n",
       " '__q': 16,\n",
       " '_qu': 16,\n",
       " 'qua': 11,\n",
       " 'uan': 7,\n",
       " 'and': 16,\n",
       " 'ndo': 18,\n",
       " 'do_': 15,\n",
       " 'cos': 5,\n",
       " 'ost': 7,\n",
       " 'str': 8,\n",
       " 'tru': 3,\n",
       " 'ruz': 2,\n",
       " 'uzi': 2,\n",
       " 'ni_': 15,\n",
       " '__a': 87,\n",
       " '_a_': 15,\n",
       " '_il': 19,\n",
       " 'il_': 17,\n",
       " 'l__': 48,\n",
       " '_c_': 1,\n",
       " 'c__': 2,\n",
       " '_sb': 1,\n",
       " 'sba': 1,\n",
       " 'bar': 3,\n",
       " 'ara': 2,\n",
       " 'raz': 1,\n",
       " 'azz': 2,\n",
       " 'zza': 5,\n",
       " 'zar': 4,\n",
       " 'art': 8,\n",
       " 'rti': 11,\n",
       " 'ti_': 43,\n",
       " '__v': 29,\n",
       " '_vo': 4,\n",
       " 'vol': 5,\n",
       " 'olt': 8,\n",
       " 'lte': 1,\n",
       " 'te_': 34,\n",
       " 'col': 8,\n",
       " 'ol_': 2,\n",
       " '__l': 67,\n",
       " '_la': 31,\n",
       " 'lav': 3,\n",
       " 'avo': 5,\n",
       " 'vor': 3,\n",
       " 'oro': 2,\n",
       " 'ro_': 9,\n",
       " 'sin': 1,\n",
       " 'uno': 2,\n",
       " 'no_': 37,\n",
       " 'ual': 8,\n",
       " 'alc': 6,\n",
       " 'lch': 1,\n",
       " 'che': 15,\n",
       " 'he_': 14,\n",
       " 'det': 2,\n",
       " 'ete': 3,\n",
       " 'tes': 5,\n",
       " 'est': 14,\n",
       " 'sta': 13,\n",
       " 'tan': 9,\n",
       " 'ano': 9,\n",
       " '_pe': 38,\n",
       " 'pen': 8,\n",
       " 'ens': 5,\n",
       " 'nsa': 3,\n",
       " 'sa_': 11,\n",
       " 'per': 32,\n",
       " 'er_': 17,\n",
       " 'r__': 18,\n",
       " '__t': 45,\n",
       " '_tr': 7,\n",
       " 'tri': 9,\n",
       " 'rib': 2,\n",
       " 'ibu': 2,\n",
       " 'bun': 2,\n",
       " 'nal': 5,\n",
       " 'ale': 12,\n",
       " 'le_': 43,\n",
       " '__n': 22,\n",
       " '_no': 15,\n",
       " 'non': 12,\n",
       " 'on_': 23,\n",
       " '_so': 16,\n",
       " 'sol': 5,\n",
       " 'olo': 7,\n",
       " 'lo_': 20,\n",
       " '_au': 3,\n",
       " 'aut': 3,\n",
       " 'uto': 2,\n",
       " 'tom': 2,\n",
       " 'omo': 3,\n",
       " 'mob': 2,\n",
       " 'obi': 2,\n",
       " 'bil': 7,\n",
       " 'ile': 3,\n",
       " '_ve': 8,\n",
       " 'ver': 16,\n",
       " 'ert': 7,\n",
       " 'tic': 6,\n",
       " 'ica': 9,\n",
       " 'cal': 2,\n",
       " 'olp': 2,\n",
       " 'lpe': 1,\n",
       " 'pe_': 1,\n",
       " 'el_': 10,\n",
       " '_ec': 2,\n",
       " 'ecc': 2,\n",
       " 'cce': 5,\n",
       " 'ces': 4,\n",
       " 'ess': 15,\n",
       " 'ssi': 7,\n",
       " 'siv': 3,\n",
       " 'iva': 7,\n",
       " 'vam': 2,\n",
       " 'ame': 8,\n",
       " 'men': 29,\n",
       " 'nte': 22,\n",
       " '_um': 2,\n",
       " 'uma': 2,\n",
       " 'man': 9,\n",
       " '_ai': 2,\n",
       " 'ai_': 10,\n",
       " '_lo': 7,\n",
       " '_ac': 6,\n",
       " 'acc': 9,\n",
       " 'cet': 2,\n",
       " 'ett': 23,\n",
       " 'tta': 10,\n",
       " 'tar': 7,\n",
       " 'are': 31,\n",
       " 're_': 79,\n",
       " '_an': 6,\n",
       " 'ans': 2,\n",
       " 'nsi': 4,\n",
       " 'sia': 2,\n",
       " 'ia_': 11,\n",
       " '__b': 11,\n",
       " '_ba': 3,\n",
       " 'bam': 2,\n",
       " 'amb': 6,\n",
       " 'mbi': 4,\n",
       " 'bin': 2,\n",
       " 'ini': 7,\n",
       " '_al': 18,\n",
       " 'lco': 1,\n",
       " 'coo': 1,\n",
       " 'ool': 1,\n",
       " '_su': 18,\n",
       " 'su_': 3,\n",
       " 'u__': 4,\n",
       " 'pri': 8,\n",
       " 'rim': 5,\n",
       " 'ime': 2,\n",
       " 'me_': 3,\n",
       " '__m': 49,\n",
       " '_mi': 6,\n",
       " 'min': 2,\n",
       " 'inu': 2,\n",
       " 'nut': 3,\n",
       " 'uti': 1,\n",
       " 'lcu': 4,\n",
       " 'cun': 4,\n",
       " '_af': 3,\n",
       " 'aff': 6,\n",
       " 'ffa': 3,\n",
       " 'far': 6,\n",
       " 'ari': 7,\n",
       " 'ri_': 16,\n",
       " 'inv': 4,\n",
       " 'nvi': 1,\n",
       " 'via': 4,\n",
       " 'iat': 5,\n",
       " 'ata': 7,\n",
       " 'ta_': 34,\n",
       " '_do': 9,\n",
       " 'dov': 5,\n",
       " 'ovr': 4,\n",
       " 'vre': 2,\n",
       " 'reb': 2,\n",
       " 'ebb': 2,\n",
       " 'bbe': 2,\n",
       " 'be_': 2,\n",
       " 'rec': 2,\n",
       " 'eci': 3,\n",
       " 'cis': 1,\n",
       " 'isi': 3,\n",
       " '__r': 24,\n",
       " '_ri': 12,\n",
       " 'ris': 5,\n",
       " 'isp': 4,\n",
       " 'spo': 4,\n",
       " 'pos': 6,\n",
       " '_da': 15,\n",
       " 'da_': 8,\n",
       " 'ing': 2,\n",
       " 'ngr': 1,\n",
       " 'gre': 2,\n",
       " 'sso': 8,\n",
       " 'so_': 7,\n",
       " '_i_': 13,\n",
       " 'sor': 1,\n",
       " 'orr': 3,\n",
       " 'rri': 4,\n",
       " 'rid': 1,\n",
       " 'idi': 2,\n",
       " 'inq': 3,\n",
       " 'nqu': 5,\n",
       " 'qui': 4,\n",
       " 'uin': 3,\n",
       " 'ina': 11,\n",
       " 'nar': 7,\n",
       " '_d_': 3,\n",
       " 'd__': 4,\n",
       " '_pa': 9,\n",
       " 'par': 6,\n",
       " 'rte': 4,\n",
       " '_mo': 12,\n",
       " 'mod': 3,\n",
       " 'odo': 2,\n",
       " '_tu': 19,\n",
       " 'tu_': 1,\n",
       " 'oll': 3,\n",
       " 'lat': 4,\n",
       " 'ate': 6,\n",
       " 'ter': 11,\n",
       " 'era': 8,\n",
       " 'ral': 2,\n",
       " 'dal': 5,\n",
       " 'all': 11,\n",
       " 'lle': 13,\n",
       " 'sui': 1,\n",
       " 'ui_': 5,\n",
       " 'tua': 8,\n",
       " 'ua_': 5,\n",
       " 'rop': 5,\n",
       " 'opr': 4,\n",
       " '_ch': 16,\n",
       " '__f': 30,\n",
       " '_fa': 11,\n",
       " 'mom': 1,\n",
       " 'ome': 3,\n",
       " 'nti': 14,\n",
       " '_ev': 3,\n",
       " 'evi': 9,\n",
       " 'vit': 4,\n",
       " 'ita': 6,\n",
       " 'dev': 6,\n",
       " 'evo': 1,\n",
       " 'von': 1,\n",
       " 'ono': 11,\n",
       " '__g': 18,\n",
       " '_gi': 5,\n",
       " 'gio': 13,\n",
       " 'iov': 3,\n",
       " 'ova': 3,\n",
       " 'van': 5,\n",
       " 'ani': 5,\n",
       " '_te': 7,\n",
       " 'err': 3,\n",
       " 'rra': 2,\n",
       " 'ra_': 8,\n",
       " 'dis': 5,\n",
       " 'isc': 3,\n",
       " 'scu': 2,\n",
       " 'cus': 3,\n",
       " 'uss': 1,\n",
       " 'sio': 4,\n",
       " '_sa': 8,\n",
       " 'sar': 5,\n",
       " 'ar√†': 1,\n",
       " 'r√†_': 4,\n",
       " '√†__': 12,\n",
       " '_be': 4,\n",
       " 'ben': 4,\n",
       " 'en_': 1,\n",
       " 'ene': 11,\n",
       " '_fe': 2,\n",
       " 'fes': 1,\n",
       " 'ten': 5,\n",
       " 'ner': 4,\n",
       " 'ere': 22,\n",
       " '_ti': 6,\n",
       " 'tip': 1,\n",
       " 'ipo': 2,\n",
       " 'po_': 6,\n",
       " '_cu': 7,\n",
       " 'cur': 4,\n",
       " 'ura': 5,\n",
       " 'sul': 4,\n",
       " 'ull': 3,\n",
       " 'll_': 3,\n",
       " 'cor': 7,\n",
       " 'orp': 2,\n",
       " 'rpo': 2,\n",
       " '_os': 2,\n",
       " 'oss': 7,\n",
       " 'sse': 10,\n",
       " 'ser': 9,\n",
       " 'erv': 2,\n",
       " 'rva': 2,\n",
       " 'var': 1,\n",
       " '_at': 2,\n",
       " 'att': 6,\n",
       " 'ttu': 4,\n",
       " 'alm': 4,\n",
       " 'lme': 6,\n",
       " 'rof': 1,\n",
       " 'ofo': 1,\n",
       " 'fon': 1,\n",
       " 'ond': 4,\n",
       " 'nda': 1,\n",
       " 'tuo': 6,\n",
       " 'uo_': 7,\n",
       " 'tue': 2,\n",
       " 'ue_': 8,\n",
       " '_pi': 14,\n",
       " 'pi√π': 7,\n",
       " 'i√π_': 7,\n",
       " '√π__': 7,\n",
       " 'nam': 3,\n",
       " 'nto': 7,\n",
       " 'to_': 36,\n",
       " '_lu': 3,\n",
       " 'luc': 2,\n",
       " 'uce': 2,\n",
       " 'ce_': 7,\n",
       " '_st': 7,\n",
       " '_fi': 13,\n",
       " 'fin': 5,\n",
       " 'ine': 4,\n",
       " '_le': 14,\n",
       " 'rea': 4,\n",
       " 'eam': 2,\n",
       " 'amp': 2,\n",
       " 'mpl': 4,\n",
       " 'pli': 3,\n",
       " 'lif': 2,\n",
       " 'ifi': 3,\n",
       " 'fic': 7,\n",
       " 'cat': 5,\n",
       " 'ato': 9,\n",
       " 'tor': 6,\n",
       " 'ore': 8,\n",
       " '_ma': 18,\n",
       " 'mas': 5,\n",
       " 'ass': 10,\n",
       " 'ssa': 8,\n",
       " 'sag': 2,\n",
       " 'agg': 9,\n",
       " 'ggi': 11,\n",
       " 'gi_': 2,\n",
       " '_po': 15,\n",
       " 'por': 15,\n",
       " 'ort': 15,\n",
       " 'rta': 10,\n",
       " 'tap': 2,\n",
       " 'apa': 2,\n",
       " 'pac': 2,\n",
       " 'cch': 2,\n",
       " 'chi': 9,\n",
       " 'hi_': 4,\n",
       " 'mag': 3,\n",
       " 'agr': 1,\n",
       " 'sie': 2,\n",
       " 'ier': 4,\n",
       " 'eri': 12,\n",
       " 'nes': 3,\n",
       " '_av': 8,\n",
       " 'avv': 1,\n",
       " 'vvo': 1,\n",
       " 'voc': 1,\n",
       " 'oca': 3,\n",
       " 'ati': 13,\n",
       " 'son': 11,\n",
       " '_ro': 2,\n",
       " 'rom': 2,\n",
       " 'oma': 2,\n",
       " 'ant': 7,\n",
       " 'ca_': 5,\n",
       " 'pol': 2,\n",
       " 'llo': 7,\n",
       " 'erg': 2,\n",
       " 'rge': 2,\n",
       " 'ger': 2,\n",
       " 'erl': 1,\n",
       " 'rla': 2,\n",
       " 'onc': 1,\n",
       " 'nce': 2,\n",
       " 'cen': 6,\n",
       " 'ntr': 7,\n",
       " 'tra': 7,\n",
       " 'ran': 8,\n",
       " 'dot': 2,\n",
       " 'oti': 5,\n",
       " '_ge': 4,\n",
       " 'gen': 3,\n",
       " '_sp': 5,\n",
       " 'spa': 4,\n",
       " 'paz': 2,\n",
       " 'io_': 18,\n",
       " 'rez': 1,\n",
       " 'ezz': 2,\n",
       " 'zzo': 2,\n",
       " 'zo_': 2,\n",
       " '_se': 15,\n",
       " 'ses': 1,\n",
       " 'ssu': 2,\n",
       " 'sua': 1,\n",
       " 'ali': 9,\n",
       " 'lit': 4,\n",
       " 'it√†': 7,\n",
       " 't√†_': 8,\n",
       " 'cco': 2,\n",
       " 'ndi': 11,\n",
       " 'sce': 3,\n",
       " 'end': 12,\n",
       " 'nde': 7,\n",
       " 'den': 4,\n",
       " '_sv': 1,\n",
       " 'svi': 1,\n",
       " 'vil': 1,\n",
       " 'ilu': 1,\n",
       " 'lup': 1,\n",
       " 'upp': 4,\n",
       " 'ppa': 2,\n",
       " 'fal': 2,\n",
       " 'acq': 1,\n",
       " 'cqu': 1,\n",
       " 'uis': 1,\n",
       " 'ist': 8,\n",
       " 'ior': 12,\n",
       " 'orn': 4,\n",
       " 'rni': 1,\n",
       " '_vi': 10,\n",
       " 'vi_': 9,\n",
       " 'ste': 4,\n",
       " 'sof': 1,\n",
       " 'off': 5,\n",
       " 'ffo': 2,\n",
       " 'foc': 2,\n",
       " 'car': 2,\n",
       " '_gl': 3,\n",
       " 'ma_': 9,\n",
       " '_us': 3,\n",
       " 'usa': 4,\n",
       " 'fat': 1,\n",
       " 'tto': 9,\n",
       " 'ang': 3,\n",
       " 'ngi': 1,\n",
       " 'gia': 5,\n",
       " 'ian': 6,\n",
       " '_du': 5,\n",
       " 'dur': 3,\n",
       " '_to': 4,\n",
       " 'ton': 1,\n",
       " '_es': 10,\n",
       " 'inc': 4,\n",
       " 'nco': 4,\n",
       " 'ont': 4,\n",
       " 'ved': 2,\n",
       " 'edi': 4,\n",
       " 'rr√†': 1,\n",
       " '_el': 3,\n",
       " 'ele': 5,\n",
       " 'lem': 7,\n",
       " 'eme': 5,\n",
       " 'dei': 2,\n",
       " 'ei_': 4,\n",
       " 'ola': 1,\n",
       " '_me': 13,\n",
       " 'eno': 3,\n",
       " 'erd': 2,\n",
       " 'rdu': 1,\n",
       " 'ure': 3,\n",
       " 'sop': 1,\n",
       " 'pra': 4,\n",
       " 'rat': 7,\n",
       " 'tut': 7,\n",
       " 'utt': 9,\n",
       " '_l_': 10,\n",
       " 'ric': 7,\n",
       " 'ice': 3,\n",
       " 'ose': 2,\n",
       " 'se_': 8,\n",
       " 'mol': 4,\n",
       " 'lti': 2,\n",
       " 'ave': 7,\n",
       " 'nos': 3,\n",
       " 'tro': 6,\n",
       " 'sal': 3,\n",
       " 'rev': 2,\n",
       " 'vis': 3,\n",
       " 'him': 1,\n",
       " 'imi': 1,\n",
       " 'mic': 3,\n",
       " 'ici': 14,\n",
       " 'ci_': 11,\n",
       " 'mig': 3,\n",
       " 'lio': 5,\n",
       " 'fag': 1,\n",
       " 'agi': 3,\n",
       " 'iol': 1,\n",
       " 'oli': 3,\n",
       " 'lin': 2,\n",
       " 'rif': 1,\n",
       " 'fiu': 1,\n",
       " 'iut': 2,\n",
       " 'ute': 1,\n",
       " 'ann': 7,\n",
       " 'nno': 8,\n",
       " 'lea': 2,\n",
       " 'ead': 1,\n",
       " 'ade': 2,\n",
       " 'der': 8,\n",
       " 'uas': 1,\n",
       " 'asi': 3,\n",
       " 'lta': 3,\n",
       " 'ers': 4,\n",
       " 'rso': 4,\n",
       " 'ona': 7,\n",
       " 'let': 4,\n",
       " 'tte': 7,\n",
       " '_ce': 8,\n",
       " 'net': 1,\n",
       " '__3': 1,\n",
       " '_30': 1,\n",
       " '30_': 1,\n",
       " '0__': 2,\n",
       " '_vu': 3,\n",
       " 'vuo': 3,\n",
       " 'uol': 1,\n",
       " 'ole': 5,\n",
       " 'ori': 10,\n",
       " '_im': 4,\n",
       " 'imp': 3,\n",
       " 'mpe': 2,\n",
       " 'peg': 1,\n",
       " 'egn': 2,\n",
       " 'gno': 2,\n",
       " '_sg': 1,\n",
       " 'sga': 1,\n",
       " 'gar': 1,\n",
       " 'arb': 1,\n",
       " 'rba': 4,\n",
       " 'bat': 1,\n",
       " 'rog': 1,\n",
       " 'ogr': 1,\n",
       " 'gra': 2,\n",
       " 'ram': 1,\n",
       " 'amm': 1,\n",
       " 'mma': 2,\n",
       " 'asc': 4,\n",
       " 'sch': 1,\n",
       " 'her': 1,\n",
       " 'arl': 6,\n",
       " 'rle': 1,\n",
       " 'due': 2,\n",
       " 'ili': 5,\n",
       " 'enn': 1,\n",
       " 'icr': 1,\n",
       " 'cro': 1,\n",
       " 'ros': 2,\n",
       " 'osc': 3,\n",
       " 'sco': 2,\n",
       " 'cop': 2,\n",
       " 'opi': 1,\n",
       " 'pio': 1,\n",
       " '_ag': 2,\n",
       " 'rna': 3,\n",
       " 'tre': 6,\n",
       " 'ret': 2,\n",
       " '_go': 1,\n",
       " 'god': 1,\n",
       " 'ode': 2,\n",
       " '_li': 2,\n",
       " 'lib': 2,\n",
       " 'ibr': 1,\n",
       " 'bro': 1,\n",
       " 'que': 8,\n",
       " 'uel': 4,\n",
       " 'lli': 2,\n",
       " 'leg': 2,\n",
       " 'egg': 3,\n",
       " 'gil': 1,\n",
       " 'pas': 4,\n",
       " '_am': 8,\n",
       " 'amo': 6,\n",
       " 'mor': 3,\n",
       " '_sm': 1,\n",
       " 'sme': 1,\n",
       " 'met': 5,\n",
       " '_br': 1,\n",
       " 'bra': 2,\n",
       " 'rac': 2,\n",
       " 'cci': 3,\n",
       " 'cia': 6,\n",
       " '_pu': 5,\n",
       " 'pu√≤': 2,\n",
       " 'u√≤_': 2,\n",
       " '√≤__': 3,\n",
       " 'sos': 3,\n",
       " 'erm': 2,\n",
       " 'rme': 2,\n",
       " 'er√†': 2,\n",
       " 'bie': 1,\n",
       " 'ien': 8,\n",
       " 'ren': 6,\n",
       " 'rip': 2,\n",
       " 'oso': 1,\n",
       " 'ind': 5,\n",
       " 'dic': 3,\n",
       " 'pet': 6,\n",
       " 'teg': 1,\n",
       " 'ego': 1,\n",
       " 'gol': 1,\n",
       " 'lez': 1,\n",
       " 'zzi': 1,\n",
       " 'zi_': 2,\n",
       " 'anu': 1,\n",
       " 'nua': 2,\n",
       " '__h': 7,\n",
       " '_ha': 7,\n",
       " 'hai': 5,\n",
       " '_bi': 2,\n",
       " 'bic': 1,\n",
       " 'cuc': 2,\n",
       " 'uci': 2,\n",
       " 'cin': 4,\n",
       " 'pot': 3,\n",
       " 'ote': 4,\n",
       " 'enz': 3,\n",
       " 'nzi': 4,\n",
       " 'zia': 6,\n",
       " 'ial': 4,\n",
       " 'lev': 1,\n",
       " 'eva': 1,\n",
       " 'vat': 2,\n",
       " '_nu': 2,\n",
       " 'utr': 2,\n",
       " 'rir': 3,\n",
       " 'irs': 1,\n",
       " 'rsi': 2,\n",
       " 'ul_': 1,\n",
       " 'dos': 2,\n",
       " 'sat': 3,\n",
       " 'ane': 3,\n",
       " '_fr': 2,\n",
       " 'fre': 2,\n",
       " 'red': 3,\n",
       " 'edd': 1,\n",
       " 'dda': 1,\n",
       " 'gel': 1,\n",
       " 'ela': 1,\n",
       " 'fas': 2,\n",
       " 'nve': 4,\n",
       " 'vec': 2,\n",
       " 'ece': 2,\n",
       " 'ena': 2,\n",
       " '_ra': 7,\n",
       " 'rag': 5,\n",
       " 'aga': 1,\n",
       " 'gaz': 1,\n",
       " 'tol': 1,\n",
       " 'los': 2,\n",
       " 'osa': 1,\n",
       " 'rit': 9,\n",
       " 'itm': 1,\n",
       " 'tmi': 1,\n",
       " 'mi_': 3,\n",
       " 'pie': 4,\n",
       " 'iet': 1,\n",
       " 'eta': 1,\n",
       " 'anz': 3,\n",
       " 'nze': 2,\n",
       " 'ze_': 2,\n",
       " 'fio': 3,\n",
       " 'sim': 1,\n",
       " 'imu': 1,\n",
       " 'mul': 2,\n",
       " 'ult': 2,\n",
       " 'nea': 1,\n",
       " 'diz': 2,\n",
       " 'izi': 9,\n",
       " 'nat': 4,\n",
       " 'com': 9,\n",
       " 'pis': 1,\n",
       " 'rad': 5,\n",
       " 'adi': 3,\n",
       " 'alu': 2,\n",
       " 'lut': 2,\n",
       " 'uta': 3,\n",
       " 'cel': 2,\n",
       " 'llu': 2,\n",
       " 'lul': 1,\n",
       " 'ula': 1,\n",
       " 'lar': 2,\n",
       " '_gr': 1,\n",
       " 'rob': 4,\n",
       " 'obl': 3,\n",
       " 'ble': 3,\n",
       " 'ema': 2,\n",
       " '_et': 1,\n",
       " 'etc': 1,\n",
       " 'tc_': 1,\n",
       " '_ar': 5,\n",
       " 'rie': 1,\n",
       " 'ie_': 5,\n",
       " 'ria': 5,\n",
       " 'lto': 4,\n",
       " 'spi': 2,\n",
       " 'pir': 2,\n",
       " 'iri': 2,\n",
       " 'ito': 5,\n",
       " 'sti': 6,\n",
       " 'tig': 2,\n",
       " 'igi': 3,\n",
       " 'mer': 3,\n",
       " 'luo': 1,\n",
       " 'uog': 1,\n",
       " 'ogh': 1,\n",
       " 'ghi': 1,\n",
       " 'suo': 2,\n",
       " 'sup': 6,\n",
       " 'upe': 3,\n",
       " 'erf': 3,\n",
       " 'rfi': 2,\n",
       " 'cie': 2,\n",
       " 'nie': 1,\n",
       " 'opp': 3,\n",
       " 'pa_': 2,\n",
       " 'san': 1,\n",
       " 'ngu': 1,\n",
       " 'gue': 1,\n",
       " 'uni': 4,\n",
       " 'stu': 4,\n",
       " 'tur': 4,\n",
       " 'urb': 3,\n",
       " 'ba_': 3,\n",
       " 'tti': 11,\n",
       " 'onv': 1,\n",
       " 'tit': 1,\n",
       " 'uoi': 5,\n",
       " 'oi_': 5,\n",
       " 'inn': 1,\n",
       " 'noc': 1,\n",
       " 'ocu': 3,\n",
       " 'cui': 4,\n",
       " 'alt': 5,\n",
       " 'ltr': 5,\n",
       " '_ps': 2,\n",
       " 'psi': 2,\n",
       " 'sic': 5,\n",
       " 'ico': 5,\n",
       " 'log': 1,\n",
       " 'ogi': 1,\n",
       " 'gic': 1,\n",
       " 'co_': 2,\n",
       " 'esp': 3,\n",
       " 'pan': 1,\n",
       " '_cr': 3,\n",
       " 'cre': 3,\n",
       " 'ear': 2,\n",
       " 'lia': 1,\n",
       " 'iar': 4,\n",
       " 'pia': 1,\n",
       " 'iac': 1,\n",
       " 'ace': 1,\n",
       " 'va_': 6,\n",
       " 'sto': 4,\n",
       " 'tos': 1,\n",
       " 'osi': 3,\n",
       " 'ima': 5,\n",
       " '_ob': 1,\n",
       " 'obb': 1,\n",
       " 'bbl': 1,\n",
       " 'bli': 1,\n",
       " 'lig': 2,\n",
       " 'igo': 1,\n",
       " 'go_': 1,\n",
       " 'erc': 9,\n",
       " 'rco': 2,\n",
       " 'sed': 1,\n",
       " 'die': 1,\n",
       " 'tat': 3,\n",
       " 'raf': 2,\n",
       " 'ffi': 4,\n",
       " 'arm': 1,\n",
       " 'rmo': 1,\n",
       " 'mon': 1,\n",
       " 'nia': 2,\n",
       " 'eli': 2,\n",
       " 'liz': 4,\n",
       " 'ues': 2,\n",
       " '_em': 3,\n",
       " 'emo': 2,\n",
       " 'moz': 1,\n",
       " 'ozi': 1,\n",
       " 'int': 2,\n",
       " 'mot': 4,\n",
       " 'tiv': 7,\n",
       " 'ivo': 4,\n",
       " 'vo_': 5,\n",
       " 'sci': 4,\n",
       " '_er': 1,\n",
       " 'dat': 4,\n",
       " '_ot': 2,\n",
       " 'ott': 3,\n",
       " 'tim': 3,\n",
       " 'vie': 2,\n",
       " 'cev': 1,\n",
       " 'eve': 2,\n",
       " 'ast': 2,\n",
       " 'pat': 1,\n",
       " 'atr': 2,\n",
       " '_re': 3,\n",
       " '_sc': 5,\n",
       " 'sca': 2,\n",
       " 'cav': 3,\n",
       " 'avi': 1,\n",
       " '_id': 2,\n",
       " 'ide': 2,\n",
       " 'dee': 1,\n",
       " 'ee_': 1,\n",
       " 'cer': 5,\n",
       " 'rch': 2,\n",
       " 'hia': 2,\n",
       " 'iam': 3,\n",
       " 'mo_': 6,\n",
       " 'dai': 1,\n",
       " '_az': 1,\n",
       " 'zie': 1,\n",
       " 'de_': 3,\n",
       " 'ana': 4,\n",
       " 'izz': 4,\n",
       " 'fil': 2,\n",
       " 'ipr': 1,\n",
       " 'esa': 2,\n",
       " '_ad': 2,\n",
       " 'ad_': 1,\n",
       " 'rem': 1,\n",
       " 'emi': 2,\n",
       " 'mio': 1,\n",
       " 'eda': 1,\n",
       " 'dec': 2,\n",
       " 'set': 2,\n",
       " 'mat': 1,\n",
       " 'ern': 1,\n",
       " 'rno': 1,\n",
       " 'arr': 2,\n",
       " 'riv': 3,\n",
       " 'ora': 3,\n",
       " 'ivi': 4,\n",
       " 'ons': 2,\n",
       " 'nse': 2,\n",
       " 'seg': 1,\n",
       " 'gna': 1,\n",
       " '_of': 4,\n",
       " 'ffr': 2,\n",
       " 'fro': 2,\n",
       " 'ron': 2,\n",
       " '_as': 3,\n",
       " 'icu': 3,\n",
       " 'aiu': 1,\n",
       " '_ta': 2,\n",
       " 'ard': 2,\n",
       " 'rdi': 1,\n",
       " 'rca': 6,\n",
       " '_ca': 6,\n",
       " 'cas': 2,\n",
       " 'asa': 2,\n",
       " 'can': 3,\n",
       " 'amu': 1,\n",
       " 'ule': 1,\n",
       " 'eto': 1,\n",
       " '_ne': 5,\n",
       " 'sun': 1,\n",
       " 'rfe': 1,\n",
       " 'fet': 2,\n",
       " 'vra': 2,\n",
       " 'rai': 2,\n",
       " 'sit': 1,\n",
       " 'itu': 3,\n",
       " 'uaz': 1,\n",
       " 'inf': 2,\n",
       " 'nfo': 2,\n",
       " 'for': 3,\n",
       " 'orm': 2,\n",
       " 'rma': 2,\n",
       " 'maz': 2,\n",
       " '_ef': 1,\n",
       " 'eff': 1,\n",
       " 'ffe': 4,\n",
       " '_uo': 1,\n",
       " 'uom': 1,\n",
       " 'fac': 2,\n",
       " 'dun': 1,\n",
       " 'unq': 2,\n",
       " 'iag': 2,\n",
       " '_or': 3,\n",
       " 'une': 2,\n",
       " '_ae': 1,\n",
       " 'aer': 1,\n",
       " 'ero': 3,\n",
       " 'opl': 1,\n",
       " 'pla': 1,\n",
       " 'lan': 1,\n",
       " 'dan': 2,\n",
       " 'nne': 4,\n",
       " 'neg': 2,\n",
       " 'idr': 1,\n",
       " 'dra': 1,\n",
       " 'ami': 2,\n",
       " 'tie': 1,\n",
       " 'onn': 3,\n",
       " 'iti': 2,\n",
       " 'ive': 1,\n",
       " 'hiu': 2,\n",
       " 'iun': 1,\n",
       " '_fu': 1,\n",
       " 'fun': 2,\n",
       " 'unz': 1,\n",
       " 'fis': 3,\n",
       " 'iss': 2,\n",
       " 'fig': 1,\n",
       " 'ppo': 3,\n",
       " 'mog': 1,\n",
       " 'ogl': 2,\n",
       " 'lie': 2,\n",
       " 'don': 2,\n",
       " 'al_': 5,\n",
       " 'mee': 1,\n",
       " 'eet': 1,\n",
       " 'eti': 1,\n",
       " 'tin': 3,\n",
       " 'ng_': 1,\n",
       " 'g__': 2,\n",
       " 'ilm': 3,\n",
       " 'lm_': 1,\n",
       " 'm__': 2,\n",
       " 'ill': 3,\n",
       " 'lum': 2,\n",
       " 'umi': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dict_count_tri_it={}#occurrence of each Italian trigram is tabulted here\n",
    "for x in tri_it:        \n",
    "    if x not in dict_count_tri_it.keys():\n",
    "        dict_count_tri_it[x]=1\n",
    "    else:\n",
    "        dict_count_tri_it[x]+=1\n",
    "        \n",
    "dict_count_tri_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to calculate the probability of the given word with respect to English trigram model.\n",
    "\n",
    "#***I have used Laplace's smoothing to improve the accuracy of the model by targetting the gram keys which are \n",
    "##                                                                                     not found in the model.\n",
    "\n",
    "# https://web.stanford.edu/~jurafsky/slp3/3.pdf explains more about Laplace smoothing which i referred for my implementation.\n",
    "\n",
    "def probability_word_tri_en(word):\n",
    "    prob=1\n",
    "    #pro_bi_word_en={}\n",
    "    tri=[word[i:i+3] for i in range (len(word)) if i!=len(word)-2]\n",
    "    #print(tri)\n",
    "    for v in tri:\n",
    "    #for bi in dict_count_bi_en.keys():\n",
    "        if v in dict_count_tri_en.keys():\n",
    "            prob*=dict_count_tri_en[v]+2/(dict_count_bi_en[v[0:2]]+(2*(len(word_train_en))))\n",
    "            #print(v,prob,dict_count_tri_en[v],dict_count_bi_en[v[0:2]],v[0:2])\n",
    "        \n",
    "    #pro_bi_word_en[word]=prob\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to calculate the probability of the given word with respect to Italian trigram model.\n",
    "\n",
    "#***I have used Laplace's smoothing to improve the accuracy of the model by targetting the gram keys which are \n",
    "##                                                                                     not found in the model.\n",
    "\n",
    "# https://web.stanford.edu/~jurafsky/slp3/3.pdf explains more about Laplace smoothing which i referred for my implementation. \n",
    "    \n",
    "def probability_word_tri_it(word):\n",
    "    prob=1\n",
    "    #pro_bi_word_en={}\n",
    "    tri=[word[i:i+3] for i in range (len(word)) if i!=len(word)-2]\n",
    "    #print(tri)\n",
    "    for v in tri:\n",
    "    #for bi in dict_count_bi_en.keys():\n",
    "        if v in dict_count_tri_it.keys():\n",
    "            prob*=dict_count_tri_it[v]+2/(dict_count_bi_it[v[0:2]]+(2*(len(word_train_it))))\n",
    "            #print(v,prob,dict_count_tri_it[v],dict_count_bi_it[v[0:2]],v[0:2])\n",
    "        \n",
    "    #pro_bi_word_en[word]=prob\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_tri_word_en={}\n",
    "for  word in word_test_en:\n",
    "    pro_tri_word_en[word]=probability_word_tri_en(word)\n",
    "    \n",
    "\n",
    "\n",
    "pro_tri_word_it={}\n",
    "for  word in word_test_en:\n",
    "    pro_tri_word_it[word]=probability_word_tri_it(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'better': 1501.014505153734,\n",
       " 'parents': 26899.358444058293,\n",
       " 'to': 1,\n",
       " 'do': 1,\n",
       " 'itchiness': 49440.78670095761,\n",
       " 'any': 12.000916590284143,\n",
       " 'cuddling': 2020.5072583803756,\n",
       " 'focus': 12.01800642926536,\n",
       " 'dry': 1.0009487666034156,\n",
       " 'are': 15.000928505106778,\n",
       " 'face': 24.0104204356321,\n",
       " 'a': 1,\n",
       " 'still': 88.0692027895529,\n",
       " 'or': 1,\n",
       " 'spouse': 32.06029828059109,\n",
       " 'with': 64.01504563113491,\n",
       " 'annual': 12.02931927860443,\n",
       " 'many': 72.01681049042332,\n",
       " 'how': 1,\n",
       " 'can': 14.000936768149883,\n",
       " 'credit': 20.041213160342853,\n",
       " 'crisis': 96.12077667840498,\n",
       " 'hills': 55.066659731917724,\n",
       " 'winning': 631.5029144935354,\n",
       " 'rate': 42.01601679827723,\n",
       " 'call': 44.014050645124506,\n",
       " 't': 1,\n",
       " 'your': 2272.0938679694186,\n",
       " 'provoke': 15.050193268948165,\n",
       " 'recall': 44.096367041941015,\n",
       " 'life': 16.00754098673754,\n",
       " 'there': 11401.878045933929,\n",
       " 'al': 1,\n",
       " 'lizard': 14.035059286987002,\n",
       " 'turn': 12.007491023983913,\n",
       " 'straight': 1352.8540464419498,\n",
       " 'is': 1,\n",
       " 'the': 76.00090334236675,\n",
       " 'and': 27.00091659028414,\n",
       " 'pursuing': 168.64003610446937,\n",
       " 'just': 10.00664410933895,\n",
       " 'attach': 20.041467416255426,\n",
       " 'it': 1,\n",
       " 'ball': 11.01136870164073,\n",
       " 'when': 42.012029194072326,\n",
       " 'some': 36.012265490764186,\n",
       " 'for': 13.000942951438,\n",
       " 'but': 1,\n",
       " 'street': 5.015029375966825,\n",
       " 'which': 8.011335073738222,\n",
       " 'this': 15.007416759657794,\n",
       " 'various': 48.12780759483348,\n",
       " 'quality': 540.738589642673,\n",
       " 're': 1,\n",
       " 'severe': 400.53538765088655,\n",
       " 'important': 577.8289163326494,\n",
       " 'in': 1,\n",
       " 'insurance': 126.50824756713926,\n",
       " 'pizzas': 16.030396636825778,\n",
       " 'maintain': 56.086546703379334,\n",
       " 'legal': 2.0009501187648455,\n",
       " 'tend': 21.00929671559584,\n",
       " 'drive': 10.016059381627414,\n",
       " 'if': 1,\n",
       " 'cushion': 120.31937414651652,\n",
       " 'throat': 15.0358406270981,\n",
       " 'carbon': 64.0978803828148,\n",
       " 'paper': 6.00664007597341,\n",
       " 'mission': 3603.7357470354336,\n",
       " 'easier': 20.022598174293798,\n",
       " 'otherwise': 9156.019418553025,\n",
       " 'places': 480.4793489599939,\n",
       " 'through': 1876.9815252162991,\n",
       " 'who': 2.000946521533365,\n",
       " 'achievements': 2875580.129877621,\n",
       " 'painkillers': 2116.037829460938,\n",
       " 'you': 71.00091996320147,\n",
       " 'court': 96.12232529460142,\n",
       " 'bake': 21.00947867936684,\n",
       " 'golf': 2.0028463023723684,\n",
       " 's': 1,\n",
       " 'about': 200.09840592926068,\n",
       " 'not': 3.0009469696969697,\n",
       " 'work': 15.007484048837433,\n",
       " 'of': 1,\n",
       " 'types': 18.019857584706372,\n",
       " 'book': 44.014152752319056,\n",
       " 'phony': 8.013159618850809,\n",
       " 'well': 2.00283963553104,\n",
       " 'that': 48.01291855780163,\n",
       " 'rather': 6845.8366006541755,\n",
       " 'irritate': 12112.031428360606,\n",
       " 'try': 3.000944733112896,\n",
       " 'make': 21.009438006184954,\n",
       " 'imagined': 769.569876978663,\n",
       " 'making': 2017.6324670153563,\n",
       " 'existential': 151034.55139672683,\n",
       " 'everything': 235852.2228964876,\n",
       " 'they': 304.072548429188,\n",
       " 'from': 9.005668066398496,\n",
       " 'few': 3.0009416195856873,\n",
       " 'opening': 5676.527313585443,\n",
       " 'quantity': 2164.625258696163,\n",
       " 'enemies': 36.10181449012797,\n",
       " 'inspires': 1795.4065004478393,\n",
       " 'areas': 375.1627406690174,\n",
       " 'academic': 4.0188761692168615,\n",
       " 'trail': 30.03880921023277,\n",
       " 'own': 3.0009478672985783,\n",
       " 'around': 160.17859918286726,\n",
       " 'puffs': 4.008534419938887,\n",
       " 'sovereign': 300.131181504154,\n",
       " 'plastic': 80.19566212843485,\n",
       " 'help': 49.01292591563006,\n",
       " 'comes': 360.14750362428083,\n",
       " 'interest': 706031.2739673217,\n",
       " 'ways': 4.004733953428424,\n",
       " 'club': 1.0009496676163343,\n",
       " 'open': 9.005669372158536,\n",
       " 'on': 1,\n",
       " 'dinner': 90.14146531165495,\n",
       " 'mouse': 16.022555532471277,\n",
       " 'child': 160.08654140745654,\n",
       " 'will': 88.01788372853848,\n",
       " 'heal': 35.01108111001919,\n",
       " 'their': 1216.5789615690112,\n",
       " 'loans': 12.015041639657376,\n",
       " 'ones': 28.01029140722073,\n",
       " 'oatmeal': 10.025401199577495,\n",
       " 'out': 8.000907441016334,\n",
       " 'left': 1,\n",
       " 'thoughts': 751.709838122586,\n",
       " 'pope': 3.0037901238559135,\n",
       " 'flirt': 8.011342362874837,\n",
       " 'complete': 2051.8651623456008,\n",
       " 'whom': 4.003782510981304,\n",
       " 'pins': 28.0102676664324,\n",
       " 'field': 27.02545587127319,\n",
       " 'trust': 5.005669738091049,\n",
       " 'soggier': 8.026580129619772,\n",
       " 'damaged': 16.053150129627902,\n",
       " 'talk': 20.008449007949103,\n",
       " 'trash': 12.018873287189765,\n",
       " 'an': 1,\n",
       " 'finished': 384.8354859438068,\n",
       " 'meditation': 908639.5862746425,\n",
       " 'monoxide': 21.08920162606167,\n",
       " 'easily': 10.025441679452275,\n",
       " 'foods': 18.0198346231682,\n",
       " 'water': 840.3065762323624,\n",
       " 'baking': 2017.636373047126,\n",
       " 'have': 60.015052087514654,\n",
       " 'hide': 7.007536096256684,\n",
       " 'milestones': 30320.659931957245,\n",
       " 'tell': 6.004681296593202,\n",
       " 'start': 35.01118239335744,\n",
       " 'trauma': 12.030354527445727,\n",
       " 'expand': 432.62961089797295,\n",
       " 'think': 70.05390294140484,\n",
       " 'n': 1,\n",
       " 'airs': 8.005670198921733,\n",
       " 'cars': 16.009302446948002,\n",
       " 'shoes': 3.000945626477541,\n",
       " 'image': 64.04539398019661,\n",
       " 'learn': 162.1089263351324,\n",
       " 'action': 6482.864682191636,\n",
       " 'too': 3.0009341429238674,\n",
       " 'together': 6858.784226458084,\n",
       " 'll': 1,\n",
       " 'soft': 2.0028248962039545,\n",
       " 'cook': 66.01598143287376,\n",
       " 'long': 6.004701623261331,\n",
       " 'pets': 1.0018863522746209,\n",
       " 'crispy': 24.04721194879998,\n",
       " 'activated': 60578.6112424016,\n",
       " 'data': 1.0018834226910796,\n",
       " 'environment': 112.23342308569283,\n",
       " 'plant': 90.10257965182622,\n",
       " 'history': 315.5904606328847}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_tri_word_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'better': 1771.460281530072,\n",
       " 'parents': 45772.58877723914,\n",
       " 'to': 1,\n",
       " 'do': 1,\n",
       " 'itchiness': 1621.1544969495947,\n",
       " 'any': 1,\n",
       " 'cuddling': 4.0037350225465165,\n",
       " 'focus': 18.020021043108827,\n",
       " 'dry': 1,\n",
       " 'are': 31.00093023255814,\n",
       " 'face': 2.0028562358280753,\n",
       " 'a': 1,\n",
       " 'still': 36.033633457524154,\n",
       " 'or': 1,\n",
       " 'spouse': 4.000951022349025,\n",
       " 'with': 1,\n",
       " 'annual': 112.08165436975341,\n",
       " 'many': 9.000942951438,\n",
       " 'how': 1,\n",
       " 'can': 3.000945626477541,\n",
       " 'credit': 72.09540690730752,\n",
       " 'crisis': 15.021868665603167,\n",
       " 'hills': 3.000942951438001,\n",
       " 'winning': 2.002781211106635,\n",
       " 'rate': 42.01219117367466,\n",
       " 'call': 22.01227018782296,\n",
       " 't': 1,\n",
       " 'your': 1,\n",
       " 'provoke': 13.013214263737206,\n",
       " 'recall': 44.08660612546235,\n",
       " 'life': 2.000940291490362,\n",
       " 'there': 22.021837437768454,\n",
       " 'al': 1,\n",
       " 'lizard': 32.030052131457424,\n",
       " 'turn': 4.000943396226415,\n",
       " 'straight': 112.08073957514685,\n",
       " 'is': 1,\n",
       " 'the': 1,\n",
       " 'and': 16.00092936802974,\n",
       " 'pursuing': 6.016123645797308,\n",
       " 'just': 1.0009546539379475,\n",
       " 'attach': 60.015019949196194,\n",
       " 'it': 1,\n",
       " 'ball': 11.00093370681606,\n",
       " 'when': 1,\n",
       " 'some': 3.000951022349025,\n",
       " 'for': 3.0009551098376313,\n",
       " 'but': 1,\n",
       " 'street': 48.059139007491865,\n",
       " 'which': 1.0018944767062727,\n",
       " 'this': 1,\n",
       " 'various': 42.05196037270029,\n",
       " 'quality': 3169.7215612165032,\n",
       " 're': 1,\n",
       " 'severe': 704.406031143752,\n",
       " 'important': 2128298.6359646367,\n",
       " 'in': 1,\n",
       " 'insurance': 320.9280021802667,\n",
       " 'pizzas': 20.008577340488536,\n",
       " 'maintain': 10.00650132746788,\n",
       " 'legal': 4.007591919869484,\n",
       " 'tend': 60.01579179392166,\n",
       " 'drive': 3.003781098742811,\n",
       " 'if': 1,\n",
       " 'cushion': 57.020829880520104,\n",
       " 'throat': 1,\n",
       " 'carbon': 2.002806971246357,\n",
       " 'paper': 32.00093501636279,\n",
       " 'mission': 1064.951099971243,\n",
       " 'easier': 24.02459541916338,\n",
       " 'otherwise': 1.0009510223490252,\n",
       " 'places': 4.008579217428831,\n",
       " 'through': 1,\n",
       " 'who': 1,\n",
       " 'achievements': 107098.4351427594,\n",
       " 'painkillers': 156.0959304688582,\n",
       " 'you': 1,\n",
       " 'court': 1,\n",
       " 'bake': 1,\n",
       " 'golf': 1.0009564801530368,\n",
       " 's': 1,\n",
       " 'about': 1,\n",
       " 'not': 1,\n",
       " 'work': 1,\n",
       " 'of': 1,\n",
       " 'types': 1.0009350163627864,\n",
       " 'book': 1,\n",
       " 'phony': 1,\n",
       " 'well': 29.000936768149884,\n",
       " 'that': 1,\n",
       " 'rather': 7.0075948154805054,\n",
       " 'irritate': 3891.757768311282,\n",
       " 'try': 1,\n",
       " 'make': 1,\n",
       " 'imagined': 180.36030479862046,\n",
       " 'making': 2.000926784059314,\n",
       " 'existential': 367677.6440535171,\n",
       " 'everything': 64.06392432830144,\n",
       " 'they': 1,\n",
       " 'from': 4.003799695699661,\n",
       " 'few': 1,\n",
       " 'opening': 16.039329436861056,\n",
       " 'quantity': 7556.166119993816,\n",
       " 'enemies': 22.03326126550175,\n",
       " 'inspires': 112.25168804093644,\n",
       " 'areas': 124.03194207746293,\n",
       " 'academic': 12.015242653722103,\n",
       " 'trail': 14.008445944079053,\n",
       " 'own': 1,\n",
       " 'around': 1,\n",
       " 'puffs': 3.000956480153037,\n",
       " 'sovereign': 704.4066703660484,\n",
       " 'plastic': 144.31776514232803,\n",
       " 'help': 1,\n",
       " 'comes': 27.011354684161578,\n",
       " 'interest': 1044369.6634390374,\n",
       " 'ways': 1,\n",
       " 'club': 1,\n",
       " 'open': 8.008562218475621,\n",
       " 'on': 1,\n",
       " 'dinner': 16.037261342192004,\n",
       " 'mouse': 1,\n",
       " 'child': 9.000946521533365,\n",
       " 'will': 3.000942951438001,\n",
       " 'heal': 1.0009546539379475,\n",
       " 'their': 1,\n",
       " 'loans': 2.0009293680297398,\n",
       " 'ones': 39.01497307425956,\n",
       " 'oatmeal': 1.0009546539379475,\n",
       " 'out': 1,\n",
       " 'left': 1,\n",
       " 'thoughts': 1,\n",
       " 'pope': 1.0009532888465205,\n",
       " 'flirt': 1.000940291490362,\n",
       " 'complete': 5191.909085905427,\n",
       " 'whom': 1,\n",
       " 'pins': 1.0009267840593141,\n",
       " 'field': 1.0009487666034156,\n",
       " 'trust': 3.0038087042904236,\n",
       " 'soggier': 44.0975088274068,\n",
       " 'damaged': 3.000942951438001,\n",
       " 'talk': 1.000931098696462,\n",
       " 'trash': 14.008445944079053,\n",
       " 'an': 1,\n",
       " 'finished': 35.01126986985492,\n",
       " 'meditation': 35613.97516947415,\n",
       " 'monoxide': 22.033179977478216,\n",
       " 'easily': 3.0009474182851728,\n",
       " 'foods': 1,\n",
       " 'water': 66.01591386013943,\n",
       " 'baking': 2.000926784059314,\n",
       " 'have': 7.000950570342205,\n",
       " 'hide': 2.0009551098376313,\n",
       " 'milestones': 6570.80566955876,\n",
       " 'tell': 29.000936768149884,\n",
       " 'start': 728.2341661381574,\n",
       " 'trauma': 14.008571161476851,\n",
       " 'expand': 16.016124932481812,\n",
       " 'think': 1,\n",
       " 'n': 1,\n",
       " 'airs': 1.0009528346831824,\n",
       " 'cars': 2.002806971246357,\n",
       " 'shoes': 1,\n",
       " 'image': 15.007570078742276,\n",
       " 'learn': 4.003771527190867,\n",
       " 'action': 19.000934142923867,\n",
       " 'too': 1,\n",
       " 'together': 1.0009510223490252,\n",
       " 'll': 1,\n",
       " 'soft': 1.000943841434639,\n",
       " 'cook': 1.00093153237075,\n",
       " 'long': 1,\n",
       " 'pets': 6.000935016362786,\n",
       " 'crispy': 20.027575888822806,\n",
       " 'activated': 588.5288247513952,\n",
       " 'data': 28.010393213238434,\n",
       " 'environment': 2381.52685549773,\n",
       " 'plant': 7.014134737122675,\n",
       " 'history': 192.09785806038693}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_tri_word_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelling of the word is done based on the probability thus obtained.\n",
    "\n",
    "dict_lang_tri_en={}\n",
    "for word in word_test_en:\n",
    "    if pro_tri_word_en[word]>pro_tri_word_it[word]:\n",
    "        dict_lang_tri_en[word]='English'\n",
    "    else:\n",
    "        dict_lang_tri_en[word]='Italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.71508379888269\n"
     ]
    }
   ],
   "source": [
    "#accuracy of the trigram model is calculated\n",
    "count_tri_en=0\n",
    "count_tri_it=0\n",
    "for x,y in dict_lang_tri_en.items():\n",
    "    if y=='English':\n",
    "        count_tri_en+=1\n",
    "        \n",
    "        \n",
    "print((count_tri_en/len(dict_lang_tri_en))*100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy of Trigram model outperforms the accuracy of the bigram model. In this case the unique bigrams thus found in english corpus are likely to be found in the italian bigrams which is ambiguous. This leads to the miss labeling of words as Italian and the accuracy is less in bigram model. In trigram model, the trigrams are quite distinct and it is not possible to have all the english trigrams in the italian set of trigrams. Thus there is less chance of missinterpretation and the accuracy improves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
